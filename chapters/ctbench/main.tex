\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Establishing a methodology for compile time benchmarking}

With libraries like Eigen\cite{eigen}, Blaze\cite{blazelib}, or CTRE\cite{ctre} being
developed with a large tempalte metaprogrammed implementation, we're seeing
increasing computing needs at compile time. These needs might grow even larger
as \cpp embeds more features over time to support and extend this kind of
practices, like compile time containers\cite{more-constexpr-containers} or static
reflection\cite{static-reflection}. However, there is still no clear cut methodology
to compare the performance impact of different metaprogramming strategies. But,
as new \cpp features allows for new techniques with claimed better compile time
performance, no proper methodologies is provided to back up those claims.

In this chapter I introduce \ctbench, which is a set of tools for compile time
benchmarking and analysis in \cpp. It aims to provide developer-friendly tools
to declare and run benchmarks, then aggregate, filter out, and plot the data to
analyze it. As such, \ctbench is meant to become the first layer of a proper
scientific methodology for analyzing compile time program behavior.

We'll first have a look at current tools for compile time profiling and
benchmarking and establish the limits of current tooling, then I'll explain
what \ctbench brings to overcome these limits.

\section{Compile time benchmarking: state of the art}

\cpp template metaprogramming raised interest for allowing computing libraries to
offer great performance with a very high level of abstraction. As a tradeoff for
interpreting representations of calculations at runtime, they are represented at
compile time, and transformed directly into their own programs.

As metaprogramming became easier with \cpp11 and \cpp17, it became more mainstream
and consequently, developers have to bear with longer compilation times without
being able to explain them. Therefore, being able to measure compilation times
is increasingly important, and being able to explain them as well. A first
generation of tools aims to tackle this issue with their own specific
methodologies:

\begin{itemize}
\item Buildbench\cite{buildbench} measures compiler execution times for basic
      A-B compile time comparisons in a web browser,
\item Metabench\cite{metabench} instantiates variably sized benchmarks using embedded
      Ruby (ERB) templating and plots compiler execution time, allowing scaling
      analyses of metaprograms,
\item Templight\cite{templight} adds Clang template instantiation inspection
      capabilities with debugging and profiling tools.
\end{itemize}

\subsection{Clang's built-int profiler}

Additionally, Clang has a built-in profiler\cite{time-trace} that provides in-depth
time measurements of various compilation steps, which can be enabled by passing
the `-ftime-trace` flag. Its output contains data that can be directly linked to
symbols in the source code, making it easier to study the impact of specific
symbols on various stages of compilation. The output format is a JSON file meant
to be compatible with Chrome's flame graph visualizer, that contains a series of
time events with optional metadata like the mangled \cpp symbol or the file
related to an event. The profiling data can then be visualized using tools such
as Google's [Perfetto UI](https://ui.perfetto.dev/).

![Perfetto UI displaying a sample Clang time trace file](docs/images/perfetto-ui.png)

Clang's profiler data is very exhaustive and insightful, however there is no
tooling to make sense of it in the context of variable size compile time
benchmarks. \ctbench tries to bridge the gap by providing a tool to analyze
this valuable data. It also improves upon existing tools by providing a solution
that's easy to integrate into existing CMake projects, and generates graphs in
various formats that are trivially embeddable in documents like research papers,
web pages, or documentations. Additionally, relying on persistent configuration,
benchmark declaration and description files provides strong guarantees for
benchmark reproductibility, as opposed to web tools or interactive profilers.

\section{ctbench functionalities}

Originally inspired by Metabench\cite{metabench}, \ctbench development was
driven by the need for a similar tool that allows the observation of Clang's
time-trace files to help get a more comprehensive view on the impact of
metaprogramming techniques on compile times. A strong emphasis was put on
developer friendliness, project integration, and component reusability.

\ctbench provides:

\begin{itemize}
\item a well documented CMake API for benchmark declaration, which can be
      generated using the \cpp pre-processor,
\item a set of JSON-configurable plotters with customizable data aggregation
      features and boilerplate code for data handling, which can be reused as a
      \cpp library.
\end{itemize}

In addition to \ctbench's time-trace handling, it has a compatibility mode
for compilers that do not support it like GCC. This mode works by measuring
compiler execution time just like Metabench\cite{metabench} and generating a
time-trace file that contains compiler execution time. Moreover, the tooling
allows setting different compilers per target within a same CMake build,
allowing black-box compiler performance comparisons between GCC and Clang for
example or comparisons between different versions of a compiler.

All these features make \ctbench a very complete toolkit for compile time
benchmarking, making comprehensive benchmark quick and easy, and the only
compile time benchmarking tool that can use Clang profiling data for metaprogram
scaling analysis.

\subsection{cmake API and compiler launcher}

In this subsection I'll discuss the CMake API and compiler launcher of \ctbench
in detail. The CMake API is responsible for generating build targets at several
sizes and with a specific layout that can be read by the

\subsection{grapher}

The \grapher component of \ctbench was developed as a reusable C++ library.

It contains all the code needed to aggregate Clang's time-trace data generated
by \ctbench benchmark targets, read JSON configuration files for plotters, and
generate plots with different plotting engines available.


\section{Statement of interest} % TODO: Rework

\ctbench was first presented at the CPPP 2021 conference\cite{ctbench-cppp21}
which is the main \cpp technical conference in France. It is being used to
benchmark examples from the poacher\cite{poacher} project, which was briefly
presented at the Meeting \cpp 2022\cite{meetingcpp22} technical conference.

\section{Related projects} % TODO: Rework

\begin{itemize}

\item Poacher (https://github.com/jpenuchot/poacher): Experimental constexpr
      parsing and code generation for the integration of arbitrary syntax DSL in
      \cpp20

\item Rule of Cheese (https://github.com/jpenuchot/rule-of-cheese):
      A collection of compile time microbenchmarks to help set better
      \cpp metaprogramming guidelines to improve compile time performance
\end{itemize}

\section{Acknowledgements}

We acknowledge contributions from Philippe Virouleau and Paul Keir for their
insightful suggestions.

\end{document}
