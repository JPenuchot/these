\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Establishing a methodology for compile time benchmarking}

With libraries like Eigen\cite{eigen}, Blaze\cite{blazelib}, or CTRE\cite{ctre} being
developed with a large tempalte metaprogrammed implementation, we're seeing
increasing computing needs at compile time. These needs might grow even larger
as \cpp embeds more features over time to support and extend this kind of
practices, like compile time containers\cite{more-constexpr-containers} or static
reflection\cite{static-reflection}. However, there is still no clear cut methodology
to compare the performance impact of different metaprogramming strategies. But,
as new \cpp features allows for new techniques with claimed better compile time
performance, no proper methodologies is provided to back up those claims.

In this chapter I introduce \ctbench, which is a set of tools for compile time
benchmarking and analysis in \cpp. It aims to provide developer-friendly tools
to declare and run benchmarks, then aggregate, filter out, and plot the data to
analyze it. As such, \ctbench is meant to become the first layer of a proper
scientific methodology for analyzing compile time program behavior.

We'll first have a look at current tools for compile time profiling and
benchmarking and establish the limits of current tooling, then I'll explain
what \ctbench brings to overcome these limits.

\section{Compile time benchmarking: state of the art}

\cpp template metaprogramming raised interest for allowing computing libraries to
offer great performance with a very high level of abstraction. As a tradeoff for
interpreting representations of calculations at runtime, they are represented at
compile time, and transformed directly into their own programs.

As metaprogramming became easier with \cpp11 and \cpp17, it became more mainstream
and consequently, developers have to bear with longer compilation times without
being able to explain them. Therefore, being able to measure compilation times
is increasingly important, and being able to explain them as well. A first
generation of tools aims to tackle this issue with their own specific
methodologies:

\begin{itemize}
\item Buildbench\cite{buildbench} measures compiler execution times for basic
      A-B compile time comparisons in a web browser,
\item Metabench\cite{metabench} instantiates variably sized benchmarks using embedded
      Ruby (ERB) templating and plots compiler execution time, allowing scaling
      analyses of metaprograms,
\item Templight\cite{templight} adds Clang template instantiation inspection
      capabilities with debugging and profiling tools.
\end{itemize}

\subsection{Clang's built-int profiler}

Additionally, Clang has a built-in profiler\cite{time-trace} that provides in-depth
time measurements of various compilation steps, which can be enabled by passing
the `-ftime-trace` flag. Its output contains data that can be directly linked to
symbols in the source code, making it easier to study the impact of specific
symbols on various stages of compilation. The output format is a JSON file meant
to be compatible with Chrome's flame graph visualizer, that contains a series of
time events with optional metadata like the mangled \cpp symbol or the file
related to an event. The profiling data can then be visualized using tools such
as Google's [Perfetto UI](https://ui.perfetto.dev/).

![Perfetto UI displaying a sample Clang time trace file](docs/images/perfetto-ui.png)

Clang's profiler data is very exhaustive and insightful, however there is no
tooling to make sense of it in the context of variable size compile time
benchmarks. \ctbench tries to bridge the gap by providing a tool to analyze
this valuable data. It also improves upon existing tools by providing a solution
that's easy to integrate into existing CMake projects, and generates graphs in
various formats that are trivially embeddable in documents like research papers,
web pages, or documentations. Additionally, relying on persistent configuration,
benchmark declaration and description files provides strong guarantees for
benchmark reproductibility, as opposed to web tools or interactive profilers.

\section{ctbench features}

Originally inspired by Metabench\cite{metabench}, \ctbench development was
driven by the need for a similar tool that allows the observation of Clang's
time-trace files to help get a more comprehensive view on the impact of
metaprogramming techniques on compile times. A strong emphasis was put on
developer friendliness, project integration, and component reusability.

\ctbench provides:

\begin{itemize}
\item a well documented CMake API for benchmark declaration, which can be
      generated using the \cpp pre-processor,
\item a set of JSON-configurable plotters with customizable data aggregation
      features and boilerplate code for data handling, which can be reused as a
      \cpp library.
\end{itemize}

In addition to \ctbench's time-trace handling, it has a compatibility mode
for compilers that do not support it like GCC. This mode works by measuring
compiler execution time just like Metabench\cite{metabench} and generating a
time-trace file that contains compiler execution time. Moreover, the tooling
allows setting different compilers per target within a same CMake build,
allowing black-box compiler performance comparisons between GCC and Clang for
example or comparisons between different versions of a compiler.

All these features make \ctbench a very complete toolkit for compile time
benchmarking, making comprehensive benchmark quick and easy, and the only
compile time benchmarking tool that can use Clang profiling data for metaprogram
scaling analysis.

\subsection{cmake API}

The decision of using CMake as a scripting language for \ctbench was rather
straightforward as it is the most widely used build system generator for \cpp
projects.

In this subsection I'll explain the CMake API. It provides functions for
generating benchmark targets at several sizes, and with a specific directory
layout that can be read by the \grapher tools. It also provides functions for
generating graph targets to compare benchmark cases.

\begin{lstlisting}[
  language=cmake,
  caption=ctbench CMake API use example,
  label=lst:ctbench-api-example
]{}
# Fine ctbench package
find_package(ctbench REQUIRED)

# Set benchmark parameters:
# range and number of samples

# From 1 to 10 with a step of 1
set(BENCH_RANGE 1 10 1
  CACHE STRING "Bench range")

# 10 samples per benchmark iteration
set(BENCH_SAMPLES 10
  CACHE STRING "Samples per iteration")

# Add time trace options to enable Clang's
# profiler output, and set the granularity
# to the tiniest value for accurate data
add_compile_options(-ftime-trace -ftime-trace-granularity=1)

# Declare benchmark targets
# with previously defined parameters

ctbench_add_benchmark_for_range(
  hello_world-flat
  hello_world/flat.cpp
  BENCH_RANGE ${BENCH_SAMPLES})

ctbench_add_benchmark_for_range(
  hello_world-pass_by_generator
  hello_world/pass_by_generator.cpp
  BENCH_RANGE ${BENCH_SAMPLES})

# Declare graph target
ctbench_add_graph(
  bfbench-consecutive_loops
  graph-config.json
  hello_world-flat
  hello_world-pass_by_generator)
\end{lstlisting}

Listing \ref{lst:ctbench-api-example} shows how to use the CMake API to generate
a graph comparing two fictional benchmark cases. Each benchmark case is
instantiated at sizes going from 1 to 10 with a step of 1, with 10 samples for
each size. These are defined in the \lstinline{BENCH_RANGE} and
\lstinline{BENCH_SAMPLES} variables, respectively.

The \lstinline{add_compile_options} function is a CMake primitive that allows
setting compile options for the target declarations that follow it. We are using
it to enable Clang's profiler, and set the granularity to its minimum (\ie 1
microsecond) for very fine data gathering.

Then we use the \lstinline{ctbench_add_benchmark_for_range} to declare two
benchmarks targets named \lstinline{hello_world-flat} and
\lstinline{hello_world-pass_by_generator}. The function takes a target name, a
\cpp source file, a range variable name, and a sample number as parameters.
A similar function called \lstinline{ctbench_add_benchmark_for_size_list} allows
defining benchmark targets using a list of sizes instead of an incremental
range.

At this point we can declare a graph target using \lstinline{ctbench_add_graph}.
It takes a target name and a path to a JSON configuration file followed by an
arbitrary number of benchmark targets as arguments. JSON configuration files are
used to pass parameters such as the plot generator name, some generic plotting
parameters such as an export format list or axis names, and generator specific
parameters like predicates for filtering or group key specifiers for aggregating
time trace events.

\subsection{compiler launcher}


% TODO: Expose the time trace file issue
% (CMake moving object files around in the background)

% TODO: Explain there is no CMake native way to handle that

% TODO: What do we do? A compiler launcher.

% TODO: What else can it do? Measure execution time of non-LLVM compilers.

\subsection{grapher}

The \grapher component of \ctbench was developed as a tookit that can be used
through executable tools, and as a C++ library as well.

It contains all the code needed to aggregate Clang's time-trace data generated
by \ctbench benchmark targets, read JSON configuration files for plotters, and
generate plots with different plot generators.

\subsubsection{File layout and C++ data structures}

% TODO: Explain data and file structures

\subsubsection{Homogeneous plotter engines}

% TODO: Introduce plotter interface

% TODO: Show some implementations

\subsubsection{Command line interface}

\begin{itemize}
\item \lstinline{ctbench-grapher-plot} % TODO
\item \lstinline{ctbench-grapher-utils} % TODO
\end{itemize}

\subsubsection{Other miscellaneous stuff}

% TODO: Error handling for JSON, statistics, and some other stuff

\section{Statement of interest} % TODO: Rework

\ctbench was first presented at the CPPP 2021 conference\cite{ctbench-cppp21}
which is the main \cpp technical conference in France. It is being used to
benchmark examples from the poacher\cite{poacher} project, which was briefly
presented at the Meeting \cpp 2022\cite{meetingcpp22} technical conference.

\section{Related projects} % TODO: Rework

\begin{itemize}

\item Poacher (https://github.com/jpenuchot/poacher): Experimental constexpr
      parsing and code generation for the integration of arbitrary syntax DSL in
      \cpp20

\item Rule of Cheese (https://github.com/jpenuchot/rule-of-cheese):
      A collection of compile time microbenchmarks to help set better
      \cpp metaprogramming guidelines to improve compile time performance
\end{itemize}

\section{Acknowledgements}

We acknowledge contributions from Philippe Virouleau and Paul Keir for their
insightful suggestions.

\end{document}
