\documentclass[main]{subfiles}
\begin{document}

This thesis is about metaprogramming techniques for parallel code generation.
It aims to study the boundary between compile-time parsing of
\glspl{dsel} and high performance code generation in \cpp.

The main motivation is to provide tools, libraries, and guidelines for embedding
mathematical languages in \cpp with the hope that it can be useful to build a
cohesive set of tools to make generic \gls{hpc} more accessible
to non-expert audiences. This goal is even more important as new computing
architectures emerge over time. Developing high performance programs requires
tuned implementations that can be achieved by either specializing
implementations for the target platforms, or using libraries that provide
specialized abstractions at various levels and for various domains.

Years after its introduction, Moore's law is still doing alive and well:
transistor count is still doubling every year despite people claiming
it is not alive anymore. This is due to a common confusion between
transistor count and computing power. While transistor count has been steadily
increasing, the throughput computing performance of single \gls{cpu} cores
has not.
This lead to the conclusion that "free lunch" is over, and future
high performance applications must adapt to increasingly parallel architectures
to increase compute throughput \cite{concurrency-revolution}.

Flynn's taxonomy \cite{flynn-taxonomy} describes four well known categories
to describe serial and parallel computer architectures\footnotemark{}:

\footnotetext{Source of the graphs:
\url{https://en.wikipedia.org/wiki/Flynn's_taxonomy}}

\begin{center}
\begin{tabular}{ p{0.3\textwidth} p{0.7\textwidth} }

\raisebox{-.925\height}{\includesvg[width=0.3\textwidth]{images/SISD}}
&
\textbf{\gls{sisd}}, where a computer only has a single processor with no
vector registers.

\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ p{0.3\textwidth} p{0.7\textwidth} }

\raisebox{-.925\height}{\includesvg[width=0.3\textwidth]{images/SIMD}}
&
\textbf{\gls{simd}}, which is a parallelism that can be implemented inside
a single \gls{cpu} core by having vector registers. These registers can hold
several values, and single instructions can operate on the vectors themselves.

\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ p{0.3\textwidth} p{0.7\textwidth} }

\raisebox{-.925\height}{\includesvg[width=0.3\textwidth]{images/MISD}}
&
\textbf{\gls{misd}}, which is primarily used for fault tolerance by having
execution redundancy.

\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ p{0.3\textwidth} p{0.7\textwidth} }

\raisebox{-.925\height}{\includesvg[width=0.3\textwidth]{images/MIMD}}
&
\textbf{\gls{mimd}}, which consists in having several processors or cores
executing different proccesses. This category of parallelism itself has several
sub-categories depending on the memory topology:

\begin{itemize}
\item \textbf{Shared memory}, where processing units share a single
pool of memory. Multi-threading is an example of such a topology as
CPU cores share the same pool of RAM.

\item \textbf{Distributed memory}, where processing units have their own
memories and cannot access other memory pools.

\item \textbf{Distributed shared memory}, where processing units have their
own memory pools and can access other memory pools indirectly.
\end{itemize}

\end{tabular}
\end{center}

This is just a high level overview of the new computer architecture landscape
high performance software engineers have to navigate in.
In an ideal world, every architecture within the same category
should be programmable with same languages, tools, and libraries.
But in reality, hardware architectures within a same category
cannot run the same programs.
For example, CUDA is not compatible with AMD \glspl{gpu}, \gls{avx} instructions
are not available on ARM processors, oneAPI \gls{gpu} libraries developed by
Intel are not compatible with its competitors' hardware, and so on.

To adapt to this variety of hardware and \glspl{api}, new abstractions
were developed in the form of software libraries, compiler extensions,
and even programming languages to help build portable high performance
applications.
Among these abstractions, a lot of them use \textbf{metaprogramming} for the
development of high performing, easy to use, and portable abstractions.

Metaprogramming is the ability to treat code as an input or output of a program.
Therefore, it allows the generation of programs specialized for
specific hardware architectures using high level, declarative \glspl{api}.
In \cpp, metaprogramming is mostly done through \gls{tmp}, which is a set of
techniques that rely on \cpp templates to represent values using types to
perform arbitrary computations at compile-time.
Type templates can even be used to represent expression trees,
which can be transformed into code. This kind of application is particularly
useful for the implementation of \cpp-based \glspl{dsel} that can build
compile-time representations of math expressions and transform them into
optimized code. Without metaprogramming, these abstractions would require
the introduction of compiler plugins or source-to-source compilers in the
compilation toolchain, making it more complex.

However, \gls{tmp} has been an esoteric practice since types are by definition
not meant to be used as values. For this reason, \cpp is evolving to provide
first class support for metaprogramming: regular \cpp code is now partially
allowed to be executed at compile-time to produce results that can be consumed
by templates. On the other hand, more \cpp libraries start relying on \cpp
metaprogramming. As such, \cpp metaprogramming becomes more mainstream,
and the amount of compile-time computing is increasing, along with
the complexity of \cpp metaprograms.
\\

% Exposing the objectives + outlining the plan

This trend, as well as new \cpp compile-time capabilities, raise new questions:
how can we leverage compile-time \cpp code execution to improve the quality
and performance of \cpp metaprograms, especially \glspl{dsel} implementations
for \acrlong{hpc} code generation?
\\

% State of the art
Before attempting to answer this question, I will present the state of the art
of metaprogramming in the context of high performance computing.
% Preliminary work: code generation for gemv
I will then present a preliminary work in which I demonstrate the viability
of \cpp metaprogramming for low-level code generation.
% Codegen techniques from \cpp ASTs
I will then present metaprogramming techniques that allow working around
\cpp limitations that would otherwise prevent the use of complex structures
generated by \gls{consteval} to be used as template parameters.
% Compile-time performance evaluation

The next part consists in implementing \glspl{dsel} using these techniques
to study them. But prior to that, I introduce a new methodology
for the scientific study of compile-time performance scaling.
This methodology is provided with an open-source toolkit for writing
\cpp compile-time benchmarks. I will then proceed to apply these
techniques to implement two \gls{dsel}:
Brainfuck as a \gls{dsel}, which features the implementation of a simple parser
and multiple code generation backends to compare several code techniques,
and a simple math language with a shunting yard based parser combined with
a code generation backend that supports high performance code generation
through existing \cpp libraries.
\\

% TODO - Quels sont les grands outils (citer les rapporteureuses)?

% TODO   - Exposer la problematique des methodes de bench pour la meta + la repro


In the first part of the thesis, we will take a look at the state of the art of
metaprogramming across many languages and in \cpp,
then we will see how it can be used to generate high performance linear algebra
kernels.

In the second part we will focus on a new methodology for the study of \cpp
metaprograms compile-time performance and use it to analyze and compare the
performance of novel \cpp metaprogramming techniques based on compile-time \cpp
execution. The study will be conducted on two embedded languages that are parsed
and transformed into \cpp code within the \cpp compilation process itself.
The first language is Brainfuck, which is provided with several code generation
backends to compare different code generation techniques. The second one
is a simple math language called Tiny Math Language that is used to demonstrate
the usability of compile-time \cpp execution to implement a \gls{dsel} for
\acrlong{hpc} application, and study the compile-time impact of this technique
in this context.

\end{document}
