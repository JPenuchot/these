@article{more-constexpr-containers,
  title = {More constexpr containers},
  author = {Peter Dimov and Louis Dionne and Nina Ranns and Richard Smith and
            Daveed Vandevoorde},
  year = {2019},
  url = {https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0784r7.html},
}

@article{clangjit,
  author = {Hal Finkel and David Poliakoff and David F. Richards},
  title = {ClangJIT: Enhancing {C++} with Just-in-Time Compilation},
  journal = {CoRR},
  volume = {abs/1904.08555},
  year = {2019},
  url = {https://arxiv.org/abs/1904.08555},
  archivePrefix = {arXiv},
  eprint = {1904.08555},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1904-08555.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{julia,
  author = {Bezanson, Jeff and Chen, Jiahao and Chung, Benjamin and Karpinski,
            Stefan and Shah, Viral B. and Vitek, Jan and Zoubritzky, Lionel},
  title = {Julia: Dynamism and Performance Reconciled by Design},
  year = {2018},
  issue_date = {November 2018},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {2},
  number = {OOPSLA},
  url = {https://doi.org/10.1145/3276490},
  doi = {10.1145/3276490},
  abstract = {Julia is a programming language for the scientific community that
              combines features of productivity languages, such as Python or
              MATLAB, with characteristics of performance-oriented languages,
              such as C++ or Fortran. Julia's productivity features include:
              dynamic typing, automatic memory management, rich type annotations,
              and multiple dispatch. At the same time, Julia allows programmers
              to control memory layout and leverages a specializing just-in-time
              compiler to eliminate much of the overhead of those features. This
              paper details the design choices made by the creators of Julia and
              reflects on the implications of those choices for performance and
              usability.},
  journal = {Proc. ACM Program. Lang.},
  month = {11},
  articleno = {120},
  numpages = {23},
  keywords = {just-in-time compilation, dynamic languages, multiple dispatch},
}

@misc{stdembed,
  author = {JeanHeyd Meneide},
  title = {P1040R6: std::embed and \#depend},
  abstract = {A proposal for a function that allows pulling resources at
              compile-time into a program.},
  url = {https://wg21.link/p1040r6},
  year = {2020},
  month = {2},
  publisher = {WG21},
}

@phdthesis{falcou-hdr,
  TITLE = {{Software Abstractions for Parallel Architectures}},
  AUTHOR = {Falcou, Joel},
  URL = {https://inria.hal.science/tel-01111708},
  SCHOOL = {{Universite de Paris 11}},
  YEAR = {2014},
  MONTH = {Dec},
  KEYWORDS = {parallel skeletons ; meta-programming ; generative programming ;
              generic programming ; parallel programming ; squelettes parall{\`e}
              les ; programmation g{\'e}n{\'e}rique ; programmation g{\'e}n{\'e}
              rative ; m{\'e}ta-programmation. ; programmation parall{\`e}le},
  TYPE = {Habilitation {\`a} diriger des recherches},
  PDF = {https://inria.hal.science/tel-01111708/file/Thesis.pdf},
  HAL_ID = {tel-01111708},
  HAL_VERSION = {v1},
}

@phdthesis{ye:tel-01061200,
  TITLE = {{Impact des transformations algorithmiques sur la synth{\`e}se de
           haut niveau : application au traitement du signal et des images}},
  AUTHOR = {Ye, Haixiong},
  URL = {https://theses.hal.science/tel-01061200},
  NUMBER = {2014PA112092},
  SCHOOL = {{Universit{\'e} Paris Sud - Paris XI}},
  YEAR = {2014},
  MONTH = {May},
  KEYWORDS = {High Level Synthesis (HLS) ; High Level Transform (HLT) ; FIR
              filter ; IIR filter ; Sigma Delta ; Morphological filter ;
              Catapult-C ; Metaprogrammation ; Signal processing ; Image
              processing ; Synth{\`e}se de haut niveau ; Transformation de haut
              niveau ; Filtre FIR ; Filtre IIR ; Sigma Delta ; Filtre
              morphologique ; Catapult-C ; M{\'e}taprogrammation ; Traitement du
              signal ; Traitement des images},
  TYPE = {Theses},
  PDF = {
         https://theses.hal.science/tel-01061200/file/VD2_YE_HAIXIONG_20052014.pdf
         },
  HAL_ID = {tel-01061200},
  HAL_VERSION = {v1},
}

@article{10.1145/243439.243447,
  author = {Jones, Neil D.},
  title = {An introduction to partial evaluation},
  year = {1996},
  issue_date = {Sept. 1996},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {28},
  number = {3},
  issn = {0360-0300},
  url = {https://doi.org/10.1145/243439.243447},
  doi = {10.1145/243439.243447},
  abstract = {Partial evaluation provides a unifying paradigm for a broad
              spectrum of work in program optimization compiling interpretation
              and the generation of automatic program generators [Bj\o{}rner et
              al. 1987; Ershov 1992; and Jones et al. 1993]. It is a program
              optimization technique, perhaps better called program
              specialization, closely related to but different from J\o{}rring
              and Scherlis' staging transformations [1986]. It emphasizes, in
              comparison with Burstall and Darlington [1977] and J\o{}rring and
              Scherlis [1986] and other program transformation work, full
              automation and the generation of program generators as well as
              transforming single programs. Much partial evaluation work to date
              has concerned automatic compiler generation from an interpretive
              definition of programming language, but it also has important
              applications to scientific computing, logic programming,
              metaprogramming, and expert systems; some pointers are given later.
              },
  journal = {ACM Comput. Surv.},
  month = {sep},
  pages = {480–503},
  numpages = {24},
  keywords = {compiler generators, compilers, interpreters, partial evaluation,
              program specialization},
}

@article{10.1023/A:1010095604496,
  author = {Futamura, Yoshihiko},
  title = {Partial Evaluation of Computation Process—AnApproach to a
           Compiler-Compiler},
  year = {1999},
  issue_date = {December 1999},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {12},
  number = {4},
  issn = {1388-3690},
  url = {https://doi.org/10.1023/A:1010095604496},
  doi = {10.1023/A:1010095604496},
  abstract = {This paper reports the relationship between formal description of
              semantics (i.e., interpreter) of a programming language and an
              actual compiler. The paper also describes a method to automatically
              generate an actual compiler from a formal description which is, in
              some sense, the partial evaluation of a computation process. The
              compiler-compiler inspired by this method differs from conventional
              ones in that the compiler-compiler based on our method can describe
              an evaluation procedure (interpreter) in defining the semantics of
              a programming language, while the conventional one describes a
              translation process.},
  journal = {Higher Order Symbol. Comput.},
  month = {dec},
  pages = {381–391},
  numpages = {11},
  keywords = {Futamura projections, compiler, interpreter, partial evaluation,
              program transformation},
}

@misc{rust-book,
  title = {The Rust Programming Language},
  author = {Steve Klabnik and Carol Nichols},
  url = {https://doc.rust-lang.org/stable/book/},
}

@misc{lisp-lang-macros,
  title = {lisp-lang.org, macros},
  url = {https://lisp-lang.org/learn/macros},
}

@article{Penuchot2023,
  doi = {10.21105/joss.05165},
  url = {https://doi.org/10.21105/joss.05165},
  year = {2023},
  publisher = {The Open Journal},
  volume = {8},
  number = {88},
  pages = {5165},
  author = {Jules Penuchot and Joel Falcou},
  title = {ctbench - compile-time benchmarking and analysis},
  journal = {Journal of Open Source Software},
}

@article{flynn-taxonomy,
  author = {Flynn, Michael J.},
  journal = {IEEE Transactions on Computers},
  title = {Some Computer Organizations and Their Effectiveness},
  year = {1972},
  volume = {C-21},
  number = {9},
  pages = {948-960},
  keywords = {Organizations;Computers;Entropy;Computational modeling;Data
              mining;Probability density function;Bandwidth;Computer
              organization;instruction stream;overlapped;parallel
              processors;resource hierarchy},
  doi = {10.1109/TC.1972.5009071},
}

@misc{sycl,
  title = {SYCL™ 2020 Specification (revision 8)},
  url = {https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html},
}

@inproceedings{6662183,
  author = {Ye, H. and Lacassagne, L. and Falcou, J. and Etiemble, D. and
            Cabaret, L. and Florent, O.},
  booktitle = {2013 23rd International Workshop on Power and Timing Modeling,
               Optimization and Simulation (PATMOS)},
  title = {High level tranforms toreduce energy consumption of signal and image
           processing operators},
  year = {2013},
  volume = {},
  number = {},
  pages = {247-254},
  keywords = {Optimization;Software;Arrays;Finite impulse response
              filters;Signal processing algorithms;Transforms;Registers;High
              Level Synthesis;High Level Transforms;algorithm transforms;software
              optimizations;ASIC;power consumption;energy optimization;signal
              processing;image processing},
  doi = {10.1109/PATMOS.2013.6662183},
}

@article{blas,
  title = {An updated set of basic linear algebra subprograms (BLAS)},
  author = {Blackford, L Susan and Petitet, Antoine and Pozo, Roldan and
            Remington, Karin and Whaley, R Clint and Demmel, James and Dongarra,
            Jack and Duff, Iain and Hammarling, Sven and Henry, Greg and others},
  journal = {ACM Transactions on Mathematical Software},
  volume = {28},
  number = {2},
  pages = {135--151},
  year = {2002},
  publisher = {Citeseer},
}

@inproceedings{matrix-matrix,
  author = "Masliah, Ian and Abdelfattah, Ahmad and Haidar, A. and Tomov, S. and
            Baboulin, Marc and Falcou, J. and Dongarra, J.",
  editor = "Dutot, Pierre-Fran{\c{c}}ois and Trystram, Denis",
  title = "High-Performance Matrix-Matrix Multiplications of Very Small Matrices
           ",
  booktitle = "Euro-Par 2016: Parallel Processing",
  year = "2016",
  publisher = "Springer International Publishing",
  address = "Cham",
  pages = "659--671",
  abstract = "The use of the general dense matrix-matrix multiplication (GEMM)
              is fundamental for obtaining high performance in many scientific
              computing applications. GEMMs for small matrices (of sizes less
              than 32) however, are not sufficiently optimized in existing
              libraries. In this paper we consider the case of many small GEMMs
              on either CPU or GPU architectures. This is a case that often
              occurs in applications like big data analytics, machine learning,
              high-order FEM, and others. The GEMMs are grouped together in a
              single batched routine. We present specialized for these cases
              algorithms and optimization techniques to obtain performance that
              is within 90 {\%} of the optimal. We show that these results
              outperform currently available state-of-the-art implementations and
              vendor-tuned math libraries.",
  isbn = "978-3-319-43659-3",
}


@inproceedings{10.1145/3564719.3568692,
  author = {Roynard, Micha\"{e}l and Carlinet, Edwin and G\'{e}raud, Thierry},
  title = {A Modern C++ Point of View of Programming in Image Processing},
  year = {2022},
  isbn = {9781450399203},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3564719.3568692},
  doi = {10.1145/3564719.3568692},
  abstract = {C++ is a multi-paradigm language that enables the programmer to
              set up efficient image processing algorithms easily. This language
              strength comes from many aspects. C++ is high-level, so this
              enables developing powerful abstractions and mixing different
              programming styles to ease the development. At the same time, C++
              is low-level and can fully take advantage of the hardware to
              deliver the best performance. It is also very portable and highly
              compatible which allows algorithms to be called from high-level,
              fast-prototyping languages such as Python or Matlab. One
              fundamental aspects where C++ shines is generic programming.
              Generic programming makes it possible to develop and reuse bricks
              of software on objects (images) of different natures (types)
              without performance loss. Nevertheless, conciliating genericity,
              efficiency, and simplicity at the same time is not trivial. Modern
              C++ (post-2011) has brought new features that made it simpler and
              more powerful. In this paper, we focus on some C++20 aspects of
              generic programming: ranges, views, and concepts, and see how they
              extend to images to ease the development of generic image
              algorithms while lowering the computation time.},
  booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on
               Generative Programming: Concepts and Experiences},
  pages = {164–171},
  numpages = {8},
  keywords = {Generic Programming, Image processing, Modern C++, Performance,
              Software},
  location = {Auckland, New Zealand},
  series = {GPCE 2022},
}

@misc{antunes2024reproducibility,
  title = {Reproducibility, Replicability, and Repeatability: A survey of
           reproducible research with a focus on high performance computing},
  author = {Benjamin A. Antunes and David R. C. Hill},
  year = {2024},
  eprint = {2402.07530},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
}

@article{doi:10.1142/S0129626404001829,
  author = {MAIGNE, LYDIA and HILL, DAVID and CALVAT, PASCAL and BRETON, VINCENT
            and REUILLON, ROMAIN and LAZARO, DELPHINE and LEGRE, YANNICK and
            DONNARIEIX, DENISE},
  title = {PARALLELIZATION OF MONTE CARLO SIMULATIONS AND SUBMISSION TO A GRID
           ENVIRONMENT},
  journal = {Parallel Processing Letters},
  volume = {14},
  number = {02},
  pages = {177-196},
  year = {2004},
  doi = {10.1142/S0129626404001829},
  URL = { https://doi.org/10.1142/S0129626404001829 },
  eprint = { https://doi.org/10.1142/S0129626404001829 },
  abstract = { Monte Carlo simulations are increasingly used in medical physics.
              In scintigraphic imaging these simulations are used to model
              imaging systems and to develop and assess tomographic
              reconstruction algorithms and correction methods for improved image
              quantization. In radiotherapy-brachytherapy the goal is to evaluate
              accurately the dosimetry in complex phantoms and at interfaces of
              tissue, where analytic calculations have shown some limits. The
              main drawback of Monte Carlo simulations is their high computing
              time. The aim of our research is to reduce the computing time by
              parallelizing a simulation on geographically distributed
              processors. The method is based on the parallelization of the
              Random Number Generator (RNG) used in Monte Carlo simulations. The
              long serial of numbers used by the sequential simulation is split.
              Once the partitioning is done, a software application allows the
              user to generate automatically the files describing each simulation
              part. Finally, another software executes them on the DataGrid
              testbed using an API. All these steps have been made transparent
              for the user by providing a web page asking the user for all the
              parameters necessary to launch the simulation and retrieve results.
              Different tests have been done in order to show first, the
              reliability of the physical results obtained by concatenation of
              parallelized output data and secondly the time gained for jobs
              execution. },
}
