@misc{hpcs1,
  title = {Math kernel library (mkl).},
  author = {Intel},
  url = {https://www.intel.com/software/products/mkl/},
}

@misc{hpcs2,
  title = {Amd core math library (acml).},
  author = {AMD},
  url = {https://developer.amd.com/amd-cpu-libraries/amd-math-library-libm/},
}

@article{hpcs3,
  author = {Zee, Field G. Van and Chan, Ernie and Geijn, Robert A. van de and
            Quintana-Ortí, Enrique S. and Quintana-Ortí, Gregorio},
  journal = {Computing in Science and Engineering},
  title = {The libflame Library for Dense Matrix Computations},
  year = {2009},
  volume = {11},
  number = {6},
  pages = {56-63},
  doi = {10.1109/MCSE.2009.207},
}

@book{hpcs4,
  author = {Cole, Murray},
  title = {Algorithmic Skeletons: Structured Management of Parallel Computation},
  year = {1991},
  isbn = {0262530864},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
}

@article{hpcs5,
  title = {Expression templates},
  author = {Veldhuizen, Todd},
  journal = {C++ Report},
  volume = {7},
  number = {5},
  pages = {26--31},
  year = {1995},
  publisher = {Citeseer},
}

@inproceedings{hpcs6,
  author = {Czarnecki, Krzysztof and {\O}sterbye, Kasper and V{\"o}lter, Markus},
  editor = {Hern{\'a}ndez, Juan and Moreira, Ana},
  title = {Generative Programming},
  booktitle = {Object-Oriented Technology ECOOP 2002 Workshop Reader},
  year = {2002},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  pages = {15--29},
  abstract = {This report describes the results of a one-day workshop on
              Generative Programming (GP) at ECOOP'02. The goal of the workshop
              was to discuss the state-of-the-art of generative techniques, share
              experience, consolidate successful techniques, and identify open
              issues for future work. This report gives a summary of the workshop
              contributions, debates, and the identified future directions.",
              isbn={978-3-540-36208-1} },
}

@inproceedings{hpcs7,
  author = {Falcou, Joel and S\'{e}rot, Jocelyn and Pech, Lucien and Laprest\'{e
            }, Jean-Thierry},
  title = {Meta-Programming Applied to Automatic SMP Parallelization of Linear
           Algebra Code},
  year = {2008},
  isbn = {9783540854500},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  url = {https://doi.org/10.1007/978-3-540-85451-7_78},
  doi = {10.1007/978-3-540-85451-7_78},
  abstract = {We describe a software solution to the problem of automatic
              parallelization of linear algebra code on multi-processor and
              multi-core architectures. This solution relies on the definition of
              a domain specific language for matrix computations, a performance
              model for multi-processor architectures and its implementation
              using C++ template meta-programming. Experimental results asses
              this model and its implementation on sample computation kernels.},
  booktitle = {Proceedings of the 14th International Euro-Par Conference on
               Parallel Processing},
  pages = {729–738},
  numpages = {10},
  location = {Las Palmas de Gran Canaria, Spain},
  series = {Euro-Par '08},
}

@book{hpcs8,
  author = {Czarnecki, Krzysztof and Eisenecker, Ulrich W.},
  title = {Generative Programming: Methods, Tools, and Applications},
  year = {2000},
  isbn = {0201309777},
  publisher = {ACM Press/Addison-Wesley Publishing Co.},
  address = {USA},
}

@book{hpcs9,
  author = {Abrahams, David and Gurtovoy, Aleksey},
  title = {C++ Template Metaprogramming: Concepts, Tools, and Techniques from
           Boost and Beyond (C++ in Depth Series)},
  year = {2004},
  isbn = {0321227255},
  publisher = {Addison-Wesley Professional},
  abstract = {C++ Template Metaprogramming sheds light on the most powerful
              idioms of today's C++, at long last delivering practical
              metaprogramming tools and techniques into the hands of the everyday
              programmer.A metaprogram is a program that generates or manipulates
              program code. Ever since generic programming was introduced to C++,
              programmers have discovered myriad "template tricks" for
              manipulating programs as they are compiled, effectively eliminating
              the barrier between program and metaprogram. While excitement among
              C++ experts about these capabilities has reached the community at
              large, their practical application remains out of reach for most
              programmers. This book explains what metaprogramming is and how it
              is best used. It provides the foundation you'll need to use the
              template metaprogramming effectively in your own work.This book is
              aimed at any programmer who is comfortable with idioms of the
              Standard Template Library (STL). C++ power-users will gain a new
              insight into their existing work and a new fluency in the domain of
              metaprogramming. Intermediate-level programmers who have learned a
              few advanced template techniques will see where these tricks fit in
              the big picture and will gain the conceptual foundation to use them
              with discipline. Programmers who have caught the scent of
              metaprogramming, but for whom it is still mysterious, will finally
              gain a clear understanding of how, when, and why it works. All
              readers will leave with a new tool of unprecedented power at their
              disposal-the Boost Metaprogramming Library.The companion CD-ROM
              contains all Boost C++ libraries, including the Boost
              Metaprogramming Library and its reference documentation, along with
              all of the book's sample code and extensive supplementary material.
              },
}

@misc{hpcs10,
  title = {Templates revisited - d programming language},
  author = {Walter Bright},
  url = {https://dlang.org/articles/templates-revisited.html},
}

@inbook{hpcs11,
  author = {Taha, Walid},
  editor = {Lengauer, Christian and Batory, Don and Consel, Charles and Odersky,
            Martin},
  title = {A Gentle Introduction to Multi-stage Programming},
  bookTitle = {Domain-Specific Program Generation: International Seminar,
               Dagstuhl Castle, Germany, March 23-28, 2003. Revised Papers},
  year = {2004},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  pages = {30--50},
  abstract = {Multi-stage programming (MSP) is a paradigm for developing generic
              software that does not pay a runtime penalty for this generality.
              This is achieved through concise, carefully-designed language
              extensions that support runtime code generation and program
              execution. Additionally, type systems for MSP languages are
              designed to statically ensure that dynamically generated programs
              are type-safe, and therefore require no type checking after they
              are generated.},
  isbn = {978-3-540-25935-0},
  doi = {10.1007/978-3-540-25935-0_3},
  url = {https://doi.org/10.1007/978-3-540-25935-0_3},
}

@inproceedings{hpcs12,
  author = {Sheard, Tim and Jones, Simon Peyton},
  title = {Template Meta-Programming for Haskell},
  year = {2002},
  isbn = {1581136056},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/581690.581691},
  doi = {10.1145/581690.581691},
  abstract = {We propose a new extension to the purely functional programming
              language Haskell that supports compile-time meta-programming. The
              purpose of the system is to support the algorithmic construction of
              programs at compile-time.The ability to generate code at compile
              time allows the programmer to implement such features as polytypic
              programs, macro-like expansion, user directed optimization (such as
              inlining), and the generation of supporting data structures and
              functions from existing data structures and functions.Our design is
              being implemented in the Glasgow Haskell Compiler, ghc.},
  booktitle = {Proceedings of the 2002 ACM SIGPLAN Workshop on Haskell},
  pages = {1–16},
  numpages = {16},
  keywords = {meta programming, templates},
  location = {Pittsburgh, Pennsylvania},
  series = {Haskell '02},
}

@inproceedings{hpcs13,
  author = {Keryell, Ronan and Reyes, Ruyman and Howes, Lee},
  title = {Khronos SYCL for OpenCL: A Tutorial},
  year = {2015},
  isbn = {9781450334846},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2791321.2791345},
  doi = {10.1145/2791321.2791345},
  abstract = {SYCL ([sikə l] as in sickle) is a royalty-free, cross-platform C++
              abstraction layer that builds on the underlying concepts,
              portability and efficiency of OpenCL, while adding the ease-of-use
              and flexibility of modern C++11. For example, SYCL enables single
              source development where C++ template functions can contain both
              host and device code to construct complex algorithms that use
              OpenCL acceleration, and then re-use them throughout their source
              code on different types of data.In this tutorial we will introduce
              the concepts behind OpenCL SYCL, present an implementation of SYCL
              targeting OpenCL devices with SPIR based on Clang/LLVM and an open
              source CPU-only implementation based on C++1z, Boost and
              OpenMP.Attendees of the last session are encouraged to install the
              open-source CPU-only implementation of SYCL and code along on
              laptop/tablet.},
  booktitle = {Proceedings of the 3rd International Workshop on OpenCL},
  articleno = {24},
  numpages = {1},
  location = {Palo Alto, California},
  series = {IWOCL '15},
}

@article{hpcs14,
  author = {Van Zee, Field G. and van de Geijn, Robert A.},
  title = {BLIS: A Framework for Rapidly Instantiating BLAS Functionality},
  year = {2015},
  issue_date = {June 2015},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {41},
  number = {3},
  issn = {0098-3500},
  url = {https://doi.org/10.1145/2764454},
  doi = {10.1145/2764454},
  abstract = {The BLAS-like Library Instantiation Software (BLIS) framework is a
              new infrastructure for rapidly instantiating Basic Linear Algebra
              Subprograms (BLAS) functionality. Its fundamental innovation is
              that virtually all computation within level-2 (matrix-vector) and
              level-3 (matrix-matrix) BLAS operations can be expressed and
              optimized in terms of very simple kernels. While others have had
              similar insights, BLIS reduces the necessary kernels to what we
              believe is the simplest set that still supports the high
              performance that the computational science community demands.
              Higher-level framework code is generalized and implemented in ISO
              C99 so that it can be reused and/or reparameterized for different
              operations (and different architectures) with little to no
              modification. Inserting high-performance kernels into the framework
              facilitates the immediate optimization of any BLAS-like operations
              which are cast in terms of these kernels, and thus the framework
              acts as a productivity multiplier. Users of BLAS-dependent
              applications are given a choice of using the traditional Fortran-77
              BLAS interface, a generalized C interface, or any other higher
              level interface that builds upon this latter API. Preliminary
              performance of level-2 and level-3 operations is observed to be
              competitive with two mature open source libraries (OpenBLAS and
              ATLAS) as well as an established commercial product (Intel MKL).},
  journal = {ACM Trans. Math. Softw.},
  month = {jun},
  articleno = {14},
  numpages = {33},
  keywords = {high-performance, Linear algebra, matrix, BLAS, libraries},
}

@inproceedings{hpcs15,
  author = {Spampinato, Daniele G. and P\"{u}schel, Markus},
  title = {A Basic Linear Algebra Compiler for Structured Matrices},
  year = {2016},
  isbn = {9781450337786},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2854038.2854060},
  doi = {10.1145/2854038.2854060},
  abstract = {Many problems in science and engineering are in practice modeled
              and solved through matrix computations. Often, the matrices
              involved have structure such as symmetric or triangular, which
              reduces the operations count needed to perform the computation. For
              example, dense linear systems of equations are solved by first
              converting to triangular form and optimization problems may yield
              matrices with any kind of structure. The well-known BLAS (basic
              linear algebra subroutine) interface provides a small set of
              structured matrix computations, chosen to serve a certain set of
              higher level functions (LAPACK). However, if a user encounters a
              computation or structure that is not supported, she loses the
              benefits of the structure and chooses a generic library. In this
              paper, we address this problem by providing a compiler that
              translates a given basic linear algebra computation on structured
              matrices into optimized C code, optionally vectorized with
              intrinsics. Our work combines prior work on the Spiral-like LGen
              compiler with techniques from polyhedral compilation to
              mathematically capture matrix structures. In the paper we consider
              upper/lower triangular and symmetric matrices but the approach is
              extensible to a much larger set including blocked structures. We
              run experiments on a modern Intel platform against the Intel MKL
              library and a baseline implementation showing competitive
              performance results for both BLAS and non-BLAS functionalities.},
  booktitle = {Proceedings of the 2016 International Symposium on Code
               Generation and Optimization},
  pages = {117–127},
  numpages = {11},
  keywords = {Program synthesis, DSL, Basic linear algebra, Tiling, Structured
              matrices, SIMD vectorization},
  location = {Barcelona, Spain},
  series = {CGO '16},
}

@article{hpcs16,
  title = {The Boost C++ metaprogramming library},
  author = {Gurtovoy, Aleksey and Abrahams, David},
  journal = {cit. on},
  pages = {22},
  year = {2002},
}

@inproceedings{hpcs17,
  author = {Estérie, Pierre and Gaunard, Mathias and Falcou, Joel and Lapresté,
            Jean-Thierry and Rozoy, Brigitte},
  booktitle = {2012 21st International Conference on Parallel Architectures and
               Compilation Techniques (PACT)},
  title = {Boost.SIMD: Generic programming for portable SIMDization},
  year = {2012},
  volume = {},
  number = {},
  pages = {431-432},
  doi = {},
}

@book{hpcs18,
  title = {LAPACK users' guide},
  author = {Anderson, Edward and Bai, Zhaojun and Bischof, Christian and
            Blackford, L Susan and Demmel, James and Dongarra, Jack and Du Croz,
            Jeremy and Greenbaum, Anne and Hammarling, Sven and McKenney, Alan
            and others},
  year = {1999},
  publisher = {SIAM},
}

@article{hpcs19,
  author = {Tomov, Stanimire and Nath, Rajib and Dongarra, Jack},
  title = {Accelerating the Reduction to Upper Hessenberg, Tridiagonal, and
           Bidiagonal Forms through Hybrid GPU-Based Computing},
  year = {2010},
  issue_date = {December, 2010},
  publisher = {Elsevier Science Publishers B. V.},
  address = {NLD},
  volume = {36},
  number = {12},
  issn = {0167-8191},
  url = {https://doi.org/10.1016/j.parco.2010.06.001},
  doi = {10.1016/j.parco.2010.06.001},
  abstract = {We present a Hessenberg reduction (HR) algorithm for hybrid
              systems of homogeneous multicore with GPU accelerators that can
              exceed 25x the performance of the corresponding LAPACK algorithm
              running on current homogeneous multicores. This enormous
              acceleration is due to proper matching of algorithmic requirements
              to architectural strengths of the system's hybrid components. The
              results described in this paper are significant because the HR has
              not been properly accelerated before on homogeneous multicore
              architectures, and it plays a significant role in solving
              non-symmetric eigenvalue problems. Moreover, the ideas from the
              hybrid HR are used to develop a hybrid tridiagonal reduction
              algorithm (for symmetric eigenvalue problems) and a bidiagonal
              reduction algorithm (for singular value decomposition problems).
              Our approach demonstrates a methodology that streamlines the
              development of a large and important class of algorithms on modern
              computer architectures of multicore and GPUs. The new algorithms
              can be directly used in the software stack that relies on LAPACK.},
  journal = {Parallel Comput.},
  month = {dec},
  pages = {645–654},
  numpages = {10},
  keywords = {Hybrid computing, Tridiagonalization, Bidiagonalization,
              Hessenberg reduction, Dense linear algebra, Two-sided
              factorizations, GPUs},
}

@article{hpcs20,
  title = {Optimizing the SVD Bidiagonalization Process for a Batch of Small
           Matrices},
  journal = {Procedia Computer Science},
  volume = {108},
  pages = {1008-1018},
  year = {2017},
  note = {International Conference on Computational Science, ICCS 2017, 12-14
          June 2017, Zurich, Switzerland},
  issn = {1877-0509},
  doi = {https://doi.org/10.1016/j.procs.2017.05.237},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050917308645},
  author = {Tingxing Dong and Azzam Haidar and Stanimire Tomov and Jack Dongarra
            },
  keywords = {Hardware accelerators, batched, two-sided factorization algorithms
              , Singular Value Problems},
  abstract = {A challenging class of problems arising in many GPU applications,
              called batched problems, involves linear algebra operations on many
              small-sized matrices. We designed batched BLAS (Basic Linear
              Algebra Subroutines) routines, and in particular the Level-2 BLAS
              GEMV and the Level-3 BLAS GEMM routines, to solve them. We proposed
              device functions and big-tile settings in our batched BLAS design.
              We adopted auto-tuning to optimize different instances of GEMV
              routines. We illustrated our batched BLAS approach to optimize
              batched bi-diagonalization progressively on a K40c GPU. The
              optimization techniques in this paper are applicable to the other
              two-sided factorizations as well.},
}

@misc{hpcs21,
  title = {OpenBLAS - An optimized BLAS library},
  author = {Zhang Xianyi and Martin Kroeker},
  url = {https://www.openblas.net},
}

@book{hpcs22,
  title = {Using the gnu compiler collection: a gnu manual for gcc version 4.3.
           3},
  author = {Stallman, Richard M},
  year = {2009},
  publisher = {CreateSpace},
}

@inproceedings{hpcs23,
  author = {Wang, Qian and Zhang, Xianyi and Zhang, Yunquan and Yi, Qing},
  booktitle = {SC '13: Proceedings of the International Conference on High
               Performance Computing, Networking, Storage and Analysis},
  title = {AUGEM: Automatically generate high performance Dense Linear Algebra
           kernels on x86 CPUs},
  year = {2013},
  volume = {},
  number = {},
  pages = {1-12},
  doi = {10.1145/2503210.2503219},
}

