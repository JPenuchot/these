\documentclass[../main]{subfiles}

\begin{document}

\section{
  ctbench features
}

ctbench implements a new methodology for the analysis of compilation times:
it allows users to define \cpp sizable benchmarks to analyze the scaling
performance of \cpp metaprogramming techniques, and compare techniques
against each other.

The project was designed to be an easy to install,
and easy to use alternative to current compile time benchmarking tools.
The public API is based on CMake as the project is aimed at \cpp software
developers and researchers. Therefore, using familiar tools
lowers barriers to entry to those who might want to contribute to it.

% It is developed with decent software quality in mind to offer a robust
% external API, and a well documented internal API to enable the reuse of
% its components.

% All these goals are kept in mind to ensure that ctbench is not just a single
% use tool for this thesis, but a sustainable open source project that enables
% researchers and developers to automatize the generation of reusable graphs
% and data artifacts.

\subsection{
  CMake API for benchmark and graph target declarations
}
\label{lbl:ctbench-cmake-api}

% The choice of using CMake as an entry point is to ensure that ctbench is not
% just a research tool made for a single set of benchmarks, but an easy-to-use
% solution for all \cpp developers willing to analyze the compile time
% performance of their metaprograms.

The public API of ctbench is meant to be as stable as reasonable by relying on
CMake as a base language. It is relatively simple, and all of it is implemented
in a single CMake script file. It declares the following:

% TODO: Nettoyer les lstinline qui depassent

\begin{itemize}

\item Benchmark declaration functions, for the declaration and instantiation
of user-defined sizable benchmark cases:

  \begin{itemize}

  \item
  \lstinline{ctbench_add_benchmark} takes a user-defined benchmark name,
  a \cpp source file, as well as benchmark range parameters: iteration begin,
  end, and step parameters, as well as the number of samples per benchmark
  iteration. Benchmark iteration targets declared using this function are
  compiled with their sized passed as the \lstinline{BENCHMARK_SIZE} define.

  \item
  \lstinline{ctbench_add_benchmark_for_range}, similar to
  \lstinline{ctbench_add_benchmark} except for the fact that benchmark range
  parameters are taken as a single list. This interface only exists to provide
  a more compact function call.

  \item
  \lstinline{ctbench_add_benchmark_for_size_list}, similar to the previous ones,
  provides a way to define benchmarks for a given size list instead of a range.

  \item
  And finally, \lstinline{ctbench_add_custom_benchmark} inherits
  \lstinline{ctbench_add_benchmark}'s interface with an addition:
  a callback function name must be passed as a parameter. It will be called
  for each benchmark iteration target definition with the name and size of the
  target. This allows users to set compiler parameters other than ctbench's
  preprocessor directive.

  \end{itemize}

\item
\lstinline{ctbench_add_graph} allows the declaration of a benchmark.
It takes a user-defined name, a path to a JSON configuration file,
an output destination, and a list of benchmark names defined using the
functions mentioned above. ctbench expects JSON files to include information
relative to the graphs themselves: the plotter being used, predicates
to filter out time trace events, output format, and so on.
Each plotter has its own set of parameters.

\item
\lstinline{ctbench_set_compiler} and \lstinline{ctbench_unset_compiler}
are a pair of commands that using different compilers for benchmarks.
CMake does not allow using more than one compiler within a CMake build,
but ctbench's compiler launcher can be used through these commands
to work around that limitation and run benchmarks for compiler performance
comparisons.

\item
The \lstinline{ctbench-graph-all} target, which allows to build all the graphs
declared with ctbench functions.

\end{itemize}

Benchmark data is laid out in folders under the following layout:
\lstinline{<benchmark target name>/<iteration size>/<repetition number>.json}.

\section{
  ctbench internal design
}

Besides the CMake scripts, ctbench is organized into several components:
\textbf{compiler-launcher}, which is a simple executable that extends CMake's
feature set, and \textbf{grapher} a \cpp library and CLI tool that reads
benchmark data and draws plots.

\subsection{
  compiler-launcher: working around CMake's limitations
}

CMake has its own limitations that would make the implementation of ctbench
impractical, or even impossible without a compiler launcher. Thankfully, CMake
does support compiler launchers through the
\lstinline{CMAKE_CXX_COMPILER_LAUNCHER} target property,
allowing us to work around those issues.

The main issue that compiler-launcher was meant to solve is the lack of
a CMake interface to retrieve the path of the binaries it generates (which is
needed to locate JSON trace files), and the lack of a Clang option to set
the output path of time trace files until Clang 16.

The functionality of the wrapper was since extended to implement other features
such as compilation time measurement for GCC or compiler permutation
to enable the use of several \cpp compilers inside a single CMake build.

\subsection{
  grapher: reading and plotting benchmark results
}

The grapher sub-project provides boilerplate code such as data structures
for benchmark data representation, and data wrangling functions for reading
benchmark data. It also serves as a framework for implementing and experimenting
new data visualization techniques that might be relevant for compile time
performance analysis.

\begin{itemize}
\item

  It provides a set of data structures is provided to handle profiling data:

  \lstinline{benchmark_instance_t} contains several data file paths to several
  repetitions of the same benchmark instantiated at the same size, as well
  as the instantiation size for all the repetition files:

\begin{lstlisting}[
  language=c++
]{}
struct benchmark_instance_t {
  unsigned size;
  std::vector<std::filesystem::path> repetitions;
};
\end{lstlisting}

  \lstinline{benchmark_case_t} contains several \lstinline{benchmark_instance_t}
  elements, as well as the name of the benchmark case:

\begin{lstlisting}[
  language=c++
]{}
struct benchmark_case_t {
  std::string name;
  std::vector<benchmark_instance_t> instances;
};
\end{lstlisting}

  And finally, \lstinline{benchmark_set_t} is used to gather
  different benchmark cases that will be compared together:

\begin{lstlisting}[
  language=c++
]{}
using benchmark_set_t =
  std::vector<benchmark_case_t>;
\end{lstlisting}

  Note that JSON data is not stored directly.
  This is intentional since a profiling file for a single benchmark repetition
  can reach volumes up to several hundreds megabytes, therefore data loading
  is delayed to prevent RAM overcomsumption.

\item

  Different plotter engines are provided, and they all follow the same
  abstraction pattern by inheriting \lstinline{plotter_base_t}.
  Currently only few plotters are considered to be usable
  (\lstinline{compare}, \lstinline{compare_by}, and \lstinline{stack}).
  Another one called \lstinline{grouped_histogram} was implemented
  for experimenting but ended up being dropped.

  The \lstinline{plotter_base_t} abstraction has a simple specification:
\begin{lstlisting}[
  language=c++
]{}
struct plotter_base_t {
  virtual ~plotter_base_t() = default;
  virtual void plot(
    benchmark_set_t const &bset,
    std::filesystem::path const &dest,
    grapher::json_t const &config) const = 0;
  virtual grapher::json_t get_default_config() const = 0;
};
\end{lstlisting}

  To implement a new plotting strategy, one simply needs to create a structure
  that implements that takes a \lstinline{benchmark_set_t} and a
  JSON configuration structure as an input.

  This level of modularity is central to the design of ctbench
  as the goal is not just to provide a benchmarking tool that is easy to use,
  but also a platform for researchers to experiment new ways to visualize
  compilation profiling data.

\item

  A JSON predicate engine is provided by the grapher library.
  It is able to convert JSON expressions into rich predicates
  that can be embedded in graph configuration files to target
  relevant compilation trace events.

\item

  The CLI interface is part of the grapher project as a supplement
  to the CMake API. It can be used to generate graphs without using
  the CMake API, or for other things such as generating default
  configuration files.

\end{itemize}

\end{document}
