\documentclass[../main]{subfiles}

\begin{document}

\section{
  ctbench design and features
}

\ctbench implements a new methodology for the analysis of compilation times:
it allows users to define \cpp sizable benchmarks to analyze the scaling
performance of \cpp metaprogramming techniques, and compare techniques
against each other.

The project was designed to be a well maintained, easy to install,
and easy to use alternative to current compile time benchmarking tools.
It is developed with decent software quality in mind to offer a robust
external API, and a well documented internal API to enable the reuse of
its components.

All these goals are kept in mind to ensure that \ctbench is not just a single
use tool for this thesis, but a sustainable open-source project that enables
researchers and developers to automatize the generation of reusable graphs
and data artifacts.

\subsection{
  CMake API for benchmark and graph target declarations
}
\label{lbl:ctbench-cmake-api}

The choice of using CMake as an entry point is to ensure that \ctbench is not
just a research tool made for a single set of benchmarks, but an easy-to-use
solution for all \cpp developers willing to analyze the compile time
performance of their metaprograms.

The public CMake API of \ctbench is meant to be as stable as reasonable.
It is relatively simple, and all of it is implemented in
a single CMake script file. It declares the following:

\begin{itemize}

\item Benchmark declaration functions, for the declaration and instantiation
of user-defined sizable benchmark cases:

  \begin{itemize}

  \item
  \lstinline{ctbench_add_benchmark} takes a user-defined benchmark name,
  a \cpp source file, as well as benchmark range parameters: iteration begin,
  end, and step parameters, as well as the number of samples per benchmark
  iteration. Benchmark iteration targets declared using this function are
  compiled with their sized passed as the \lstinline{BENCHMARK_SIZE} define.

  \item
  \lstinline{ctbench_add_benchmark_for_range}, similar to
  \lstinline{ctbench_add_benchmark} except for the fact that benchmark range
  parameters are taken as a single list. This interface only exists to provide
  a more compact function call.

  \item
  \lstinline{ctbench_add_benchmark_for_size_list}, similar to the previous ones,
  provides a way to define benchmarks for a given size list instead of a range.

  \item
  And finally, \lstinline{ctbench_add_custom_benchmark} inherits
  \lstinline{ctbench_add_benchmark}'s interface with an addition:
  a callback function name must be passed as a parameter. It will be called
  for each benchmark iteration target definition with the name and size of the
  target. This allows users to set compiler parameters other than \ctbench's
  preprocessor directive.

  \end{itemize}

\item
\lstinline{ctbench_add_graph} allows the declaration of a benchmark.
It takes a user-defined name, a path to a JSON configuration file,
an output destination, and a list of benchmark names defined using the
functions mentioned above. \ctbench expects JSON files to include information
relative to the graphs themselves: the plotter being used, predicates
to filter out time trace events, output format, and so on.
Each plotter has its own set of parameters.

\item
\lstinline{ctbench_set_compiler} and \lstinline{ctbench_unset_compiler}
are a pair of commands that using different compilers for benchmarks.
CMake does not allow using more than one compiler within a CMake build,
but \ctbench's compiler launcher can be used through these commands
to work around that limitation and run benchmarks for compiler performance
comparisons.

\item
The \lstinline{ctbench-graph-all} target, which allows to build all the graphs
declared with \ctbench functions.

\end{itemize}

Benchmark data is laid out in folders under the following layout:
\lstinline{<benchmark target name>/<iteration size>/<repetition number>.json}.

\subsection{
  ctbench internals
}

Besides the CMake scripts, \ctbench is organized into several components:

\begin{itemize}

\item
\textbf{A CMake API}: a CMake script file is installed as part of the CMake
package. It provides an implement of the CMake API previously described.

\item
\textbf{grapher}: a \cpp library and CLI tool that reads benchmark data
and draws plots. It

\item
\textbf{compiler-launcher}: a simple program that handles compiler flags in
order to move JSON profiling output to the right place for grapher
to retrieve it, measures compiler execution time if required, and invokes the
desired compiler as part of the \lstinline{ctbench_set_compiler} CMake
command implementation.

\end{itemize}

\subsubsection{
  grapher: reading and plotting benchmark results
}

The grapher sub-project provides boilerplate code such as data structures
for benchmark data representation, and data wrangling functions for reading
benchmark data. It also serves as a framework for implementing and experimenting
new data visualization techniques that might be relevant for compile time
performance analysis.

\begin{itemize}
\item

  It provides a set of data structures is provided to handle profiling data:

  \lstinline{benchmark_instance_t} contains several data file paths to several
  repetitions of the same benchmark instantiated at the same size, as well
  as the instantiation size for all the repetition files:

\begin{lstlisting}[
  language=c++
]{}
struct benchmark_instance_t {
  unsigned size;
  std::vector<std::filesystem::path> repetitions;
};
\end{lstlisting}

  \lstinline{benchmark_case_t} contains several \lstinline{benchmark_instance_t}
  elements, as well as the name of the benchmark case:

\begin{lstlisting}[
  language=c++
]{}
struct benchmark_case_t {
  std::string name;
  std::vector<benchmark_instance_t> instances;
};
\end{lstlisting}

  And finally, \lstinline{benchmark_set_t} is used to gather
  different benchmark cases that will be compared together:

\begin{lstlisting}[
  language=c++
]{}
using benchmark_set_t =
  std::vector<benchmark_case_t>;
\end{lstlisting}

  Note that JSON data is not stored directly.
  This is intentional since a profiling file for a single benchmark repetition
  can reach volumes up to several hundreds megabytes, therefore data loading
  is delayed to prevent RAM overcomsumption.

\item

  Different plotter engines are provided, and they all follow the same
  abstraction pattern by inheriting \lstinline{plotter_base_t}.
  Currently only few plotters are considered to be usable
  (\lstinline{compare}, \lstinline{compare_by}, and \lstinline{stack}).
  Another one called \lstinline{grouped_histogram} was implemented
  for experimenting but ended up being dropped.

  The \lstinline{plotter_base_t} abstraction has a simple specification:
\begin{lstlisting}[
  language=c++
]{}
struct plotter_base_t {
  virtual ~plotter_base_t() = default;
  virtual void plot(
    benchmark_set_t const &bset,
    std::filesystem::path const &dest,
    grapher::json_t const &config) const = 0;
  virtual grapher::json_t get_default_config() const = 0;
};
\end{lstlisting}

  To implement a new plotting strategy, one simply needs to create a structure
  that implements that takes a \lstinline{benchmark_set_t} and a
  JSON configuration structure as an input.

  This level of modularity is central to the design of ctbench
  as the goal is not just to provide a benchmarking tool that is easy to use,
  but also a platform for researchers to experiment new ways to visualize
  compilation profiling data.

\item

  A JSON predicate engine is provided by the grapher library.
  It is able to convert JSON expressions into rich predicates
  that can be embedded in graph configuration files to target
  relevant compilation trace events.

\item

  The CLI interface is part of the grapher project as a supplement
  to the CMake API. It can be used to generate graphs without using
  the CMake API, or for other things such as generating default
  configuration files.

\end{itemize}

\subsubsection{
  compiler-launcher: working around CMake's limitations
}

CMake has its own limitations that would make the implementation of \ctbench
impractical, or even impossible without a compiler launcher. Thankfully, CMake
does support compiler launchers through the
\lstinline{CMAKE_CXX_COMPILER_LAUNCHER} target property,
allowing us to work around those issues.

The main issue that compiler-launcher was meant to solve is the lack of
a CMake interface to retrieve the path of the binaries it generates (which is
needed to locate JSON trace files), and the lack of a Clang option to set
the output path of time trace files until Clang 16.

The functionality of the wrapper was since extended to support more use cases.

\begin{itemize}
\item find out where json files are located with \lstinline{-ftime-trace}
\item CLI option parsing
\item compiler execution time measurement
\end{itemize}

\subsection{
  Software quality
}

An emphasis was put on software quality since the beginning of the project.
As mentioned, the goal was not just to develop a plotting tool for a single
compilation time analysis, but to provide a methodology along with a robust
implementation to improve compilation time analysis as a whole.

\begin{itemize}

\item
The use of common \cpp development tools for the project facilitates
makes it easier for users to contribute to the project.

\item
The use of a GitHub CI for building and testing the project guarantees that
project remains functional at all times on all supported platforms.
As of writing, the project is continuously built and tested for Ubuntu 23.04
and Arch Linux. The test environment is fully reproducible as well thanks to
the use of Docker for its setup.

\item
compiler checks: clang-tidy and clang-format

\item
Packages are provided through the AUR and the vcpkg repository

\item
Accepted in JOSS, which puts an emphasis on software quality.
the review itself is meant to improve the quality of the software
thanks to the reviewers' feedback.
\end{itemize}

\subsection{
  Conclusion
}

\ctbench is a well

\end{document}
