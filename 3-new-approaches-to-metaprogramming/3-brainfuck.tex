\documentclass[../main]{subfiles}
\begin{document}

\section{
  Brainfuck parsing and code generation
}
\label{lbl:bf-parsing-and-codegen}

Now that we introduced the various techniques to generate programs from
pointer trees generated by \gls{constexpr} functions, we will use them in the
context of compile time parsing and code generation for the Brainfuck language.
Therefore use data structures and code generation techniques introduced in
subsection \ref{lbl:ptr-tree-codegen}.

\subsection{
  Constexpr Brainfuck parser and AST
}

\subsubsection{
  AST
}

The Brainfuck \gls{ast} is defined in the header shown in appendix \ref{app:bf-ast}.
The header file also contains helper function definitions to handle \gls{ast} nodes
safely, such as \lstinline{visit} which will be used in one of the
code generation backends.

Here are the main data types:

\begin{itemize}
\item
\lstinline{node_interface_t}, which is a common base type for all \gls{ast} nodes.

\item
\lstinline{ast_token_t}, which represents a single \gls{ast} token.

\item
\lstinline{ast_block_t}, which represents an \gls{ast} block, which simply is a
\lstinline{std::vector} of \lstinline{std::unique_ptr<node_interface_t>}.

\item
\lstinline{ast_while_t}, which represents a while conditional block.
The instruction block itself is contained in an \lstinline{ast_block_t} value.

\end{itemize}

The implementation of the \gls{ast} are available in appendix \ref{app:bf-ast}
where you can observe that all the types are implemented as they would be
for a regular Brainfuck parser, except all their methods are \gls{constexpr}.

\begin{lstlisting}[
  language=c++,
  caption=Definition of the \gls{ast} visitor function,
  label=lst:bf-ast-visit-def
]{}
template <typename F>
constexpr auto visit(F f, ast_node_ptr_t const &p) {
  switch (p->get_kind()) {
  case ast_token_v:
    return f(static_cast<ast_token_t const &>(*p));
  case ast_block_v:
    return f(static_cast<ast_block_t const &>(*p));
  case ast_while_v:
    return f(static_cast<ast_while_t const &>(*p));
  }
}
\end{lstlisting}

The \lstinline{visit} function implementation also looks like a regular \cpp
function as shown in listing \ref{lst:bf-ast-visit-def}.
It is a higher order function that allows recursive operations on the \gls{ast}
to be carried in a type-safe manner.

\subsubsection{
  Parser
}

The Brainfuck parser, again, looks like nothing special. For that reason I will
not get into the implementation details. The function definition is available
in appendix \ref{app:bf-parser}.

On the surface: the parser takes a pair of begin and end iterators as an input.
It parses Brainfuck tokens until it reaches the end iterator or a while end
token, and returns a pair containing an iteretor pointing after the last parsed
token and the parsing result.

When a while begin token is reached, it calls itself recursively and resumes
parsing at the position of the iterator returned by the callee, which is right
after the while block.

The main parsing function implementation (including the function prototype)
is very condensed: it fits in 40 lines of code with a max line width set to 84.
It is no different from a regular Brainfuck parsing function except for it being
\gls{constexpr}, and it can actually be used as a regular \cpp program.

These make it much easier to debug as it can be ran through a \cpp debugger like
GDB or LLDB, and also more maintainable as it does not require any
template metaprogramming experience to understand the implementation.
Additionally, \gls{constexpr} execution enforces checks on memory allocations and
deallocations as well as memory bound checking. Therefore testing functions
in \gls{constexpr} contexts can help finding memory safety issues.

\subsection{
  A variety of techniques to generate code from a dynamic AST
}

Once the \gls{constexpr} parser is implemented, the next step consists in
figuring out how to transform its result, which contains dynamic memory,
into \cpp code.

As you may remember from subsection \ref{lbl:constexpr-programming},
there is no direct way to use values holding pointers to dynamic memory
directly as \glspl{nttp}.
Therefore it must be conveyed by other means or transformed into \glspl{litval}
to be used as template parameters for \cpp code generation.

I implemented several of these workarounds to compare them.
This will give us a clearer idea of their implementation difficulty,
and they will enable us to run compilation time benchmarks to compare their
compilation time performance.

\subsubsection{Pass-by-generator + ET}

The first technique I implemented was passing \gls{ast} nodes through lambdas
to convert them into expression templates.

\subsubsection{Pass-by-generator}

actually very hard to implement because \lstinline{std::unique_pointer}
lifetime management gets more difficult when combined with
\gls{constexpr} constraints.

% TODO: Mention compilation times

\subsubsection{Serializing the \gls{ast} into a \gls{litval}}

The last technique I will discuss, which is by far the most efficient
in regard to compilation time, consists in transforming the \gls{ast} into a
\gls{litval} that can be used as a \gls{nttp}.

As I've discussed in \ref{lbl:constexpr-programming}, a \gls{constexpr}
\lstinline{std::vector} return value can be transformed into a static array with
a trivial transformation. As long as a static array holds \glspl{litval},
the array itself remains a \gls{litval} as well, and can therefore be used
as a \gls{nttp}.

In order to do this, an intermediate serialized representation of the \gls{ast}
which must only contain \glspl{litval} must be implemented.
It is a fairly trivial task:

\begin{itemize}
\item
Polymorphism through the inheritance of \lstinline{node_interface_t}
can be replaced with the use of \lstinline{std::variant}, which is literal
as long as the value it holds is literal as well.

\item
Pointers contained in AST nodes can be replaced by integer indexes pointing to
internal values of the array (static or dynamic) in which serialized
AST nodes are stored.
\end{itemize}

\begin{lstlisting}[
  language=c++,
  caption=\gls{pbg} intermediate representation type definitions,
  label=lst:flat-ir-typedef
]{}
/// Represents a single instruction token
struct flat_token_t {
  token_t token;
};

/// Block descriptor at the beginning of every block
/// of adjacent instructions.
struct flat_block_descriptor_t {
  size_t size;
};

/// Represents a while instruction, pointing to
/// another instruction block
struct flat_while_t {
  size_t block_begin;
};

/// Polymorphic representation of a node
using flat_node_t =
    std::variant<flat_token_t, flat_block_descriptor_t,
                 flat_while_t>;

/// AST container type
using flat_ast_t = std::vector<flat_node_t>;

/// NTTP-compatible AST container type
template <size_t N>
using fixed_flat_ast_t = std::array<flat_node_t, N>;
\end{lstlisting}

In listing \ref{lst:flat-ir-typedef}, I show how this was implemented in the
backend that will be used for the benchmarks.
Note that the serialized \gls{ast} can be stored either as a
\lstinline{flat_ast_t} for convenience in \glspl{consteval},
which can then be transformed into a \lstinline{fixed_flat_ast_t<N>}
to be passed as a \gls{nttp}.

To serialize the AST, we will rely on a function called \lstinline{block_gen}
which visits the AST recursively and generates serializes the AST progressively.

\begin{lstlisting}[
  language=c++,
  caption=\lstinline{block_gen_state_t} definition,
  label={lst:flat-block_gen_state_t}
]{}
/// Support structure for generate_blocks function
struct block_gen_state_t {
  /// Flat AST blocks, result of block_gen
  std::vector<flat_ast_t> blocks;

  /// Keeping track of which block is being generated
  size_t block_pos = 0;

  /// Keeping track of the size of the AST
  size_t total_size = 0;
};
\end{lstlisting}

In listing \ref{lst:flat-block_gen_state_t} we begin by defining a structure
called \lstinline{block_gen_state_t} that contains the intermediate result.

\begin{lstlisting}[
  language=c++,
  caption=Generic \lstinline{block_gen} implementation,
  label=lst:flat-block_gen-generic
]{}
/// Extracts an AST into a vector of blocks of
/// contiguous operations
constexpr void block_gen(ast_node_ptr_t const &,
                         block_gen_state_t &);

// Node-specific overloads...

constexpr void block_gen(ast_node_ptr_t const &p,
                         block_gen_state_t &s) {
  visit([&s](auto const &v) { block_gen(v, s); }, p);
}
\end{lstlisting}

Listing \ref{lst:flat-block_gen-generic} shows the implementation of the generic
version of \lstinline{block_gen}. It consists in a forward declaration
followed by node-specific implementations which call the generic version
recursively. The generic version relies on the \lstinline{visit} function,
and for that reason overloads for derivatives of \lstinline{node_interface_t}
must be declared first. To make it simpler, I chose to define them as well
to avoid unnecessary forward declarations.

\begin{lstlisting}[
  language=c++,
  caption=\lstinline{block_gen} implementation,
  label={lst:flat-block_gen-impl}
]{}
/// block_gen for a single AST token. Basically just a
/// push_back.
constexpr void block_gen(ast_token_t const &tok,
                         block_gen_state_t &s) {
  s.blocks[s.block_pos].push_back(
      flat_token_t{tok.token});
}

/// block_gen for an AST block.
constexpr void block_gen(ast_block_t const &blo,
                         block_gen_state_t &s) {
  // Save & update block pos before adding a block
  size_t const previous_pos = s.block_pos;
  s.block_pos = s.blocks.size();
  s.blocks.emplace_back();

  // Preallocating
  s.blocks[s.block_pos].reserve(
      blo.content.size() + 1);
  s.total_size += blo.content.size() + 1;

  // Adding block descriptor as a prefix
  s.blocks[s.block_pos].push_back(
      flat_block_descriptor_t{
          blo.content.size()});

  // Flattening instructions
  for (ast_node_ptr_t const &node :
       blo.content) {
    block_gen(node, s);
  }

  // Restoring block pos after recursive block
  // processing
  s.block_pos = previous_pos;
}

/// block_gen for a while instruction.
constexpr void block_gen(ast_while_t const &whi,
                         block_gen_state_t &s) {
  s.blocks[s.block_pos].push_back(
      flat_while_t{s.blocks.size()});
  block_gen(whi.block, s);
}
\end{lstlisting}

Listing \ref{lst:flat-block_gen-impl} shows the implementation of
\lstinline{block_gen} overloads for all types derived from
\lstinline{node_interface_t}.

The most important one is the implementation for \lstinline{ast_block_t}.
This is the overload that generates new blocks in the \lstinline{blocks} vector
of the \lstinline{block_gen_state_t} structure.

It works by simply allocating a new block in the \lstinline{block_gen_state_t}
structure, and traverse each node by calling \lstinline{block_gen} recursively.
This ensures the elements contained in \gls{ast} block nodes remain contiguous
which is instrumental to make code generation easier later on.

The elements contained in these blocks have their indexes pointing to
the elements of the \lstinline{blocks} vector
of the \lstinline{block_gen_state_t} structure, but the expected result is a
single \lstinline{std::vector} containing all \gls{ast} nodes.

The \lstinline{flatten} function is responsible for transforming the
contents of \lstinline{block_gen_state_t} into such a representation.

\begin{lstlisting}[
  language=c++,
  caption=\lstinline{flatten} implementation,
  label={lst:flat-flatten-impl}
]{}
constexpr flat_ast_t
flatten(ast_node_ptr_t const &parser_input) {
  flat_ast_t serialized_ast;

  // Extracting as vector of blocks
  block_gen_state_t bg_result;
  block_gen(parser_input, bg_result);

  // Small optimization to avoid reallcations
  serialized_ast.reserve(bg_result.total_size);

  // block_map[i] gives the index of
  // bg_result.blocks[i] in the serialized
  // representation
  std::vector<size_t> block_map;
  block_map.reserve(bg_result.blocks.size());

  // Step 1: flattening
  for (flat_ast_t const &block : bg_result.blocks) {
    // Updating block_map
    block_map.push_back(serialized_ast.size());

    // Appending instructions
    std::ranges::copy(
        block, std::back_inserter(serialized_ast));
  }

  // Step 2: linking
  for (flat_node_t &node : serialized_ast) {
    if (std::holds_alternative<flat_while_t>(node)) {
      flat_while_t &w_ref =
          std::get<flat_while_t>(node);
      w_ref.block_begin =
          block_map[w_ref.block_begin];
    }
  }

  return serialized_ast;
}
\end{lstlisting}

Listing \ref{lst:flat-flatten-impl} shows the implementation of
\lstinline{flatten}, which simply calls \lstinline{block_gen},
chains the blocks together, and translates block indexes to account for
the new layout.

The postfix serialization layout (ie. the fact that blocks are laid out
one after the other, and not one inside the other) ensures that
traversing the \gls{ast} remains trivial. Note that derializing the blocks
into an infix representation would essentially get us back to the
unparsed representation of Brainfuck programs.

From there, the only thing that needs to be done is to evaluate this
dynamic array into a static one to make it usable as an \gls{nttp}.

\begin{lstlisting}[
  language=c++,
  caption=\lstinline{parse_to_fixed_flat_ast} implementation,
  label={lst:flat-parse-to-fixed-flat-ast-implementation}
]{}
/// Parses a BF program into a fixed_flat_ast_t value.
template <auto const &ProgramString>
constexpr auto parse_to_fixed_flat_ast() {
  // Getting AST vector size into a constexpr variable
  constexpr size_t AstArraySize =
      flatten(parser::parse_ast(ProgramString))
          .size();

  // Initializing static size array
  fixed_flat_ast_t<AstArraySize> arr;
  std::ranges::copy(
      flatten(parser::parse_ast(ProgramString)),
      arr.begin());

  return arr;
}
\end{lstlisting}

Listing \ref{lst:flat-parse-to-fixed-flat-ast-implementation} shows the
implementation of the final function that takes a string as a program
and transforms it into a \lstinline{fixed_flat_ast_t}.

So far, this is the only function that takes a template parameter as an input,
and thus where the distinction between \gls{constexpr} programming and \gls{tmp}
begins.

All the remaining work consists in implementing a function that takes the
serialized \gls{ast} as a template parameter and generates a program from it.

\begin{lstlisting}[
  language=c++,
  caption=\lstinline{program_state_t} definition,
  label=lst:program_state_t-def
]{}
struct program_state_t {
  constexpr program_state_t() : i(0) {
    for (auto &c : data) {
      c = 0;
    }
  }

  std::array<char, 30000> data;
  std::size_t i;
};
\end{lstlisting}

The functions generated by the codegen functions must take a
\lstinline{program_state_t} as a reference to execute Brainfuck code.
The program state structure definition is shown in listing
\ref{lst:program_state_t-def}.

The implementation principle for the code generation functions is to visit
nodes recusively to compose and generate lambdas.
We will explore two ways to implement the \gls{ast} traversal:
one based on a monolithic implementation based on function overloading
to differentiate each type of node, and another one based on
\lstinline{if constexpr}.

% Monolithic vs overloaded backend implementation
\begin{itemize}
\item
The overloaded version consists in a code generation entry point function
that selects instruction-specific code generation functions depending
on the type of token stored in the current instruction.

\begin{lstlisting}[
  language=c++,
  caption=Overloaded \lstinline{codegen} entry point function,
  label=lst:ol-codegen-entrypoint
]{}
// Necessary forward declaration
template <auto const &Ast,
          size_t InstructionPos = 0>
constexpr auto codegen();

// Specialization implementations...

/// Generic code generation entrypoint
template <auto const &Ast, size_t InstructionPos>
constexpr auto codegen() {
  constexpr flat_node_t Instr =
      Ast[InstructionPos];

  // Calling specialized codegen versions
  // using function overloading
  return codegen<Ast, InstructionPos>(
      decltype(get<Instr.index()>(Instr)){});
}
\end{lstlisting}

Listing \ref{lst:ol-codegen-entrypoint} shows the implementation
of the entry point function whose only purpose is to unwrap the type
of the current element and use it to call the appropriate code generation
function depending on the instruction type.
The dispatch is done automatically though function overloading.

\begin{lstlisting}[
  language=c++,
  caption={
    \lstinline{codegen} specialization for \lstinline{flat_token_t} elements
  },
  label=lst:ol-codegen-token
]{}
/// Code generation implementation
/// for a single instruction
template <auto const &Ast,
          size_t InstructionPos = 0>
constexpr auto codegen(flat_token_t) {
  // Extracting token value
  constexpr flat_token_t Token =
      get<flat_token_t>(Ast[InstructionPos]);

  // Returning code for a single Brainfuck
  // instruction

  // >
  if constexpr (Token.token ==
                pointer_increase_v) {
    return [](program_state_t &s) { ++s.i; };
  }
  // <
  else if constexpr (Token.token ==
                     pointer_decrease_v) {
    return [](program_state_t &s) { --s.i; };
  }

  // More instructions...
}
\end{lstlisting}

In listing \ref{lst:ol-codegen-token} we can see the implementation of a
code generation function for simple instructions, \ie any instruction
represented by a token that isn't a while block delimiter.

The dispatch over the tokens is done using \lstinline{if constexpr},
but it could have been done using a variable template with
a series of specializations.

Note that the token passed as a regular parameter is only used for the overload
selection. The token value used to evaluate conditions in the
\lstinline{if constexpr} statements is sourced from the \gls{ast} passed as a
\gls{nttp}, therefore it is still \gls{constexpr}.

\begin{lstlisting}[
  language=c++,
  caption={
    \lstinline{codegen} specialization
    for \lstinline{flat_block_descriptor_t} elements
  },
  label=lst:ol-codegen-block
]{}
/// Code generation implementation
/// for a code block
template <auto const &Ast,
          size_t InstructionPos = 0>
constexpr auto codegen(flat_block_descriptor_t) {
  return [](program_state_t &s) {
    // Generating an index sequence type
    // with a size equal to the code block size.
    // It will be passed to the template lambda
    // to expand its indexes.
    auto index_sequence =
        std::make_index_sequence<
            get<flat_block_descriptor_t>(
                Ast[InstructionPos])
                .size>{};

    // Static unrolling on the block's
    // instructions, made possible by the
    // contiguity of its elements
    [&]<size_t... Indexes>(
        std::index_sequence<Indexes...>) {
      // Expansion on the index to generate code
      // for each node and invoke it with the
      // program state
      (..., codegen<Ast, 1 + InstructionPos +
                             Indexes>()(s));
    }(index_sequence);
  };
}
\end{lstlisting}

The trickiest part of code generation is generating code blocks, as shown in
listing \ref{lst:ol-codegen-block}. Doing so requires the use of a compile time
unrolling technique based on \cpp parameter packs.
Iteration over the elements must be done that way to keep the index
\gls{constexpr} and use it as a \gls{nttp} to generate code.

The result of this metafunction is an anonymous function that evaluates all the
Brainfuck code within a block. At this point the only function that remains
is the \lstinline{flat_while_t} overload.

\begin{lstlisting}[
  language=c++,
  caption={
    \lstinline{codegen} specialization
    for \lstinline{flat_while_t} elements
  },
  label=lst:ol-codegen-while
]{}
/// Code generation implementation
/// for a while block
template <auto const &Ast,
          size_t InstructionPos = 0>
constexpr auto codegen(flat_while_t) {
  return [](program_state_t &s) {
    while (s.data[s.i]) {
      codegen<Ast, get<flat_while_t>(
                       Ast[InstructionPos])
                       .block_begin>()(s);
    }
  };
}
\end{lstlisting}

The \lstinline{codegen} implementation for a while block
\ref{lst:ol-codegen-while} is trivial: it returns a function that runs a while
loop as defined by the Brainfuck language specification, and the body itself is
the code generation result for the block element it points to.

This implementation is a good way to show how code generation from a \gls{nttp}
can be implemented for a serialized \gls{ast}.

\item
A monolithic implementation of \lstinline{codegen} can be implemented by
replacing overloading with \lstinline{if constexpr} and
\lstinline{std::holds_alternative}.

\begin{lstlisting}[
  language=c++,
  caption={\lstinline{if constexpr} based \lstinline{codegen} implementation},
  label=lst:icx-codegen
]{}
template <auto const &Ast,
          size_t InstructionPos = 0>
constexpr auto codegen() {
  constexpr flat_node_t Instr =
      Ast[InstructionPos];

  if constexpr (holds_alternative<flat_token_t>(
                    Instr)) {
    constexpr flat_token_t Token =
        get<flat_token_t>(Instr);

    // Single token code generation...
  }

  else if constexpr (holds_alternative<
                         flat_block_descriptor_t>(
                         Instr)) {
    constexpr flat_block_descriptor_t
        BlockDescriptor =
            get<flat_block_descriptor_t>(Instr);
    // Block code generation...
  }

  else if constexpr (holds_alternative<
                         flat_while_t>(Instr)) {
    constexpr flat_while_t While =
        get<flat_while_t>(Instr);
    // While loop code generation...
  }
}
\end{lstlisting}

Listing \ref{lst:icx-codegen} shows the \lstinline{if constexpr} based
code generation implementation. Note that bits of code were cut to make
the listing shorter, but they are the same as the overloaded \lstinline{codegen}
overload function bodies. This \lstinline{codegen} implementation remains
functionally identical to the previous one.

\end{itemize}

\subsubsection{
  Difficulties
}

\begin{itemize}
\item Embedding text from the original string
\item \lstinline{if constexpr} requiring the definition of a \gls{constexpr} variable
\end{itemize}

\subsubsection{
  Implementation complexity
}

\subsubsection{
  Small synthetic variable-size benchmarks
}

We first begin by running two variable-sized benchmarks, consisting in
measuring compiler execution time as the \gls{ast} widens, and as the \gls{ast} deepens.

The first variable-sized benchmark consists in generating a valid BF \gls{ast} by
concatenating strings to generate a succession of BF while loops in a
\gls{constexpr} string. This benchmark was instantiated with sizes going from 1 to
10 with a step of 1, with 10 timing iterations for each size.

The second benchmark generates a string with
nested loops, making the \gls{ast} deeper as the benchmark size increases instead
of making it wider as in the previous case.

Both benchmarks generate programs of the same size so comparisons can be made
properly.

\begin{figure}
\includegraphics[scale=0.5]{images/bf-consecutive_loops.png}
\includegraphics[scale=0.5]{images/bf-imbricated_loops.png}
\includegraphics[scale=0.5]{images/bf-graph-legend.png}
\caption{
  Compiler execution time measurements for consecutive loops (left)
  and nested loops (right)
}\label{fig:bf-bench}
\end{figure}

Figure \ref{fig:bf-bench}
both highlight considerably higher compiler execution times for the expression
template based backend, high enough to suggest that the use of expression
templates induces an overhead higher than parsing and generating Brainfuck
programs using the \gls{pbg} backend. However the \gls{pbg}
backend still has a compile time overhead much higher than the flat backend,
which shows near constant compiler execution times on these small scale
benchmarks.

Finally, \gls{ast} deepening has a much higher impact on compile times than \gls{ast}
widening with the expression template backends, whereas the other backends seem
to scale similarly as the \gls{ast} grows wider or deeper.

\subsubsection{
  Large Brainfuck programs
}

The following benchmarks consist in measuring compiler execution times for
compiling Brainfuck code examples. These example programs are also used to
validate the metacompiler's backend implementations by compiling them and
verifying their output.

\begin{itemize}
\item A Hello World program (106 tokens).
\item The same Hello World program, ran twice (212 tokens).
\item A Mandelbrot set fractal viewer (11672 tokens).
\end{itemize}

\begin{figure}
\begin{tabular}{|c|c|c|c|}
\hline
Backend           & Hello World & Hello World x2  & Mandelbrot \\
\hline
Flat (Monolithic) & 0.63        & 0.80            & 18.16 \\
Flat (Overloaded) & 0.66        & 0.90            & 28.51 \\
\gls{pbg}         & 3.55        & 12.73           & Failure (timeout) \\
\gls{et}          & 19.18       & 74.51           & Failure (timeout) \\
\hline
\end{tabular}
\caption{Brainfuck compile time measurements in seconds}
\label{fig:bf-compile-times}
\end{figure}

The measurements in figure \ref{fig:bf-compile-times} help us better assess
how various metacompiling techniques behave at scale.
The Hello World program is meant to represent a simple embedded expression,
while the Mandelbrot visualization is a much larger program meant to represent
an upper bound of what a \gls{dsel} would be reasonably used for as it is more
than 200 lines of code.

However, proposals such as \lstinline{std::embed} \cite{stdembed}
could dramatically increase the size of embedded programs as whole source files
could be used directly as embedded expressions or programs in \cpp.

\subsubsection{
  Conclusions
}

Here is what can be said about each backend:

\begin{itemize}

\item
\textbf{Flat backends}:
they are not the easiest to implement due to the additional intermediate
representation and serialization step that need to be implemented.

However, they present clear benefits when it comes to compilation times.
So far they are the only ones that can be used at scale.
The Mandelbrot example is supposed to illustrate an extreme case where
\glsplural{dsel} are used to integrate large programs (approximately 11'000
\gls{ast} nodes), and yet both "flat" implementations manage to keep compilation
times well under a minute.

\item
\textbf{\gls{pbg} backend}:
The \gls{pbg} backend has the shortest implementation.
However, it might not be the least difficult to implement.

The code examples do not reflect the time spent debugging \gls{constexpr}
allocated memory errors. While the \gls{pbg} may be a decent route for
rapid prototyping to compile simple embedded expressions or programs,
its poor performance scaling might be problematic.

Judging from the implementation, the initial hypothesis was that its
compilation time complexity as a function of program size would be quadratic
and was confirmed by a compilation time analysis.
This hypothesis was confirmed by compilation time benchmarks on small
and large scale programs.

\item
\textbf{\gls{et} backend}:
This backend was originally meant to be a demonstrator for the interoperability
of \gls{constexpr} memory and \glsplural{et}. It shows no particular advantage
compared to the two other backends: it is slower than the \gls{pbg} backend while
requiring additional effort to implement the \gls{et}
intermediate representation.

\end{itemize}

As a broad conclusion for compile time Brainfuck code generation,
it can be said that using \gls{nttp}-based techniques to generate code
is preferable to avoid compilation times increasing dramatically.

\end{document}
