\documentclass[../main]{subfiles}
\begin{document}

% - SotA metaprog (Can't read)
%   - C++ (Can't read)
%   - Reflection
%   - Can't read

In this state of the art I will first give an overview of metaprogramming in
historic and contemporary languages. Then I will focus on the state of the art
of \cpp metaprogramming, and notable high performance computing libraries as they
are essential for the scope of my thesis.

\section{
  Metaprogramming styles and languages
}

Metaprogramming is not an old concept. It exists in many forms:
both the C preprocessor and \cpp templates can be used to take code as input,
and generate code as well.
However it is clear that these two mechanisms are completely different.
The C preprocessor can only read and write tokens with very basic language,
while \cpp templates can manipulate types and complex \cpp values
with a much richer semantic.

Our goal in this section will be to cover different languages with their own
metaprogramming models, as well as \gls{hpc}-focused libraries that use
metaprogramming to achieve notable levels of genericity.

\subsection{A short history of metaprogramming}

% TODO

\subsection{Metaprogramming in contemporary languages}

Metaprogramming perpetuates itself in contemporary languages,
with some being more widespread than others.

\paragraph{MetaOCaml}

MetaOCaml\cite{metaocaml} implements quoting and splicing
\ie the ability to essentially copy and paste expressions,
as well as staged compilation to evaluate statements at compile-time.
This enables code generation to occur both at runtime and at compile-time.

\paragraph{DLang} more or less extends the \cpp Metaprogramming model.
It leverages templates and compile time function evaluation just like
its predecessor.

Compile-time evaluation is much more permissive and mixins enable to generate
code in a more direct way than \cpp. Dlang mixins allow injecting code in
functions and structures in two ways: using template mixins
which are pre-parsed constructs that can be injected later,
as well as string mixins that allow strings containing Dlang code
to be compiled and inserted directly into programs.

\paragraph{Rust} proposes metaprogramming through macros, generics, and traits.

Rust macros are more powerful than those proposed in C and \cpp.
They have a

\paragraph{Braid} \cite{braid} is language that implements metaprogramming
through multi-staged programming (like MetaOCaml) for heterogeneous real-time
3D graphics. It is however unmaintained.

\paragraph{Julia} \cite{julia} is a dynamic language that uses the LLVM

\paragraph{Terra}

Terra\cite{terra} implements a very explicit metaprogramming model.
The language is based on LUA, and exploits the dynamic nature of the language
together with LLVM \gls{jit} compilation to allow code generation
to happen at runtime.
It implements multi-staged compilation and splicing just like MetaOCaml.

Additionally, Terra can be embedded in other languages through its C API.
Overall it is a very versatile and experimental take on metaprogramming,
but the lack of interoperability with \cpp templates makes it hard to justify
its use for \gls{hpc} applications.

As we will see in section \ref{lbl:meta-cpp}, \gls{gpu} computing libraries
rely heavily on \cpp metaprogramming to provide building blocks for
portable high performance compute kernels.

\section{
  Metarpogramming and \gls{hpc} libraries
}
\label{lbl:meta-libraries}

\subsection{Core and application-specific libraries}

As previously said \cpp templates can be seen as a functional language.
Over time a range of libraries emerged, aiming to provide functionalities
similar to regular language such as containers and algorithms for use in
template metaprograms. Notable examples of such libraries are MPL\cite{mpl},
Brigand\cite{brigand}, and mp11\cite{mp11}.

Libraries for more specific uses were also implemented, such as
Spirit\cite{spirit} for writing parsers (not for compile time parsing),
\gls{ctre} \cite{ctre} for compiling regular expressions,
and \gls{ctpg} \cite{ctpg} for generating LR1 parsers
(also not for compile time parsing).

The benefits of metaprogrammed libraries are:

\begin{itemize}

\item Performance: notably in the case of \gls{ctre}.
Regular expressions are usually interpreted at runtime,
which adds a measurable overhead to text processing.
\gls{ctre} shows leading performance, on par with Rust's regex library
which also works by compiling regular expressions.

\item Language integration: since these are \cpp libraries,
their APIs can take advantage of \cpp operator overloading and lambdas.
In \gls{ctpg}, these are used to provide a domain-specific language that is close to
what parser generators like YACC or Bison provide,
though it is still regular \cpp code which can be put inside any function body.
Using a \cpp API makes these libraries easier to learn
as the syntax is already familiar to their users.

\item Streamlined toolchain: as they only require to be included as headers.
This avoids complicating compilation toolchains by requiring additional programs
to be installed and integrated to the build system.

\end{itemize}

\subsection{High performance computing libraries}

There are many \cpp \gls{hpc} libraries that use metaprogramming to achieve
high levels of performance while being generic and proposing high level APIs.
This subsection aims to provide an overview of notable libraries that fit this
description as well as recent efforts that are still under development.

\begin{itemize}

\item
\textbf{Eigen} \cite{eigen} is the first major \cpp library to implement
Expression templates for the generation of high performance math computing.
Expression templates is a \cpp design pattern that consists in representing
math expressions with type template trees. We will discuss them later
in \ref{lbl:expression-level-metaprogramming}.

\begin{lstlisting}[
  language=c++
]{}
#include <Eigen/Dense>
#include <iostream>

using Eigen::MatrixXd;
using Eigen::VectorXd;

int main() {
  MatrixXd m = MatrixXd::Random(3, 3);
  m = (m + MatrixXd::Constant(3, 3, 1.2)) * 50;
  std::cout << "m =" << std::endl << m << std::endl;
  VectorXd v(3);
  v << 1, 2, 3;
  std::cout << "m * v =" << std::endl
            << m * v << std::endl;
}
\end{lstlisting}

\item
\textbf{Blaze} \cite{blazelib} is a successor of Eigen that implements so-called
"Smart Expression Templates" which extends upon the concept of
expression templates implemented by Eigen. It aims to provide a more performant
and extensible \gls{hpc} library. However, Eigen is not set in stone
and its designed has since been updated.

\begin{lstlisting}[
  language=c++
]{}
#include <blaze/Math.h>
#include <iostream>

using blaze::DynamicVector;
using blaze::StaticVector;

int main() {
  StaticVector<int, 3UL> a{4, -2, 5};
  DynamicVector<int> b(3UL);

  b[0] = 2;
  b[1] = 5;
  b[2] = -3;

  DynamicVector<int> c = a + b;

  std::cout << "c =\n" << c << "\n";
}
\end{lstlisting}

\item
\textbf{NT2} \cite{nt2} is a research project that aims to provide a complete
numerical toolbox that leverages metaprogramming to develop portable \gls{hpc}
applications with a Matlab-like interface while still achieving state-of-the-art
computing performance.

\begin{lstlisting}[
  language=c++
]{}
#include <nt2/include/functions/ones.hpp>
#include <nt2/table.hpp>

using namespace nt2;

int main() {
  table<double> x;
  table<double> y = ones(4, 4);

  x = 40.0 * y + 2.0;

  NT2_DISPLAY(x);

  return 0;
}
\end{lstlisting}

\item
\textbf{EVE} \cite{eve} provides generic abstractions over SIMD instructions
as well as SIMD-optimized generic algorithms for the development of
high performance and portable SIMD code \cite{hpcs2018-matvec}.

\begin{lstlisting}[
  language=c++
]{}
#include <eve/module/core.hpp>
#include <eve/wide.hpp>
#include <iostream>

int main() {
  eve::wide<float> x(
      [](auto i, auto) { return 1.f + i; });
  std::cout << "x     = " << x << "\n";
  std::cout << "2*x   = " << x + x << "\n";
  std::cout << "x^0.5 = " << eve::sqrt(x) << "\n";

  return 0;
}
\end{lstlisting}

\item
\textbf{HPX} \cite{hpx} is a \cpp parallel and distributed runtime library.
It can execute small parallel tasks efficiently and distribute
larger distributed tasks with a work following data execution model.
Its parallel and distributed APIs as well as its parallel implementation of
the standard library (based on its own parallel runtime) use metaprogramming
for algorithmic genericity.

\begin{lstlisting}[
  language=c++
]{}
std::uint64_t fibonacci(std::uint64_t n) {
  if (n < 2)
    return n;

  hpx::future<std::uint64_t> n1 =
      hpx::async(fibonacci, n - 1);
  hpx::future<std::uint64_t> n2 =
      hpx::async(fibonacci, n - 2);

  // wait for the futures to return their values
  return n1.get() + n2.get();
}
\end{lstlisting}

\item
\textbf{Thrust} \cite{thrust} implements \acrshort{gpu}-accelerated equivalents
of the Standard Library's algorithms, while CUB \cite{cub} provides
\acrshort{gpu}-optimized algorithm skeletons for generic programming on NVIDIA \gls{gpu}\acrpluralsuffix{}.
AMD and Intel implement their equivalents for their own platforms, respectively
ROCm and OneAPI.

\begin{lstlisting}[
  language=c++
]{}
#include <algorithm>
#include <thrust/device_vector.h>
#include <thrust/functional.h>
#include <thrust/generate.h>
#include <thrust/host_vector.h>
#include <thrust/reduce.h>

int main(void) {
  // generate random data serially
  thrust::host_vector<int> h_vec(100);
  std::generate(h_vec.begin(), h_vec.end(), rand);

  // transfer to device and compute sum
  thrust::device_vector<int> d_vec = h_vec;
  int x = thrust::reduce(d_vec.begin(), d_vec.end(),
                         0, thrust::plus<int>());
  return 0;
}
\end{lstlisting}

\item
\textbf{faer} \cite{faer}

\begin{lstlisting}[
  language=c++
]{}
use faer::{mat, Mat, prelude::*};

// empty 0x0 matrix
let m0: Mat<f64> = Mat::new();

// zeroed 4x3 matrix
let m1: Mat<f64> = Mat::zeros(4, 3);

// 3x3 identity matrix
let m2 = Mat::from_fn(
  3, 3,
  |i, j| if i == j { 1.0 } else { 0.0 });

// 4x2 matrix with custom data
let m3 = mat![
    [4.93, 2.41],
    [5.43, 4.33],
    [9.83, 1.59],
    [7.13, 5.02_f64],
];

// compute the qr decomposition of a matrix
let qr_decomposition = m3.qr();
\end{lstlisting}

\end{itemize}

\section{
  Conclusion
}

All these examples show that metaprogramming is not just a gimmick.
There is enough interest in leveraging metaprogramming
for generic \gls{hpc} programming that many frameworks, libraries, or languages
have been developed around this paradigm and maintained sometimes
for more than a decade. Its adoption in languages like Braid, Julia, Terra,
Rust, or Dlang shows that it is not limited to \cpp.

However, most of generic \gls{hpc} libraries and frameworks that are developed
and maintained today are based on \cpp, and \cpp itself is likely to
evolve to provide more ways to generate programs as shown by the recent
standard proposal for Scalable Reflection in \cpp \cite{scalable-reflection}.

\begin{itemize}
\item Metaprogramming isn't a new idea

\item Some languages provide advanced metaprogramming capabilities

\item \cpp has solid metaprogramming constructs, and a complete \gls{hpc} ecosystem
(libraries, compilers, etc.)
\end{itemize}

\end{document}
