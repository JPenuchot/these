\documentclass[../main]{subfiles}
\begin{document}


\gls{blas} level functions\cite{blas} are the cornerstone of a large
subset of applications. If a large body of work surrounding
efficient and large-scale implementation of some routines such
as \gls{gemv} exists, the interest for small-sized, highly-optimized
versions of those routines emerged.

In this chapter, we show that modern \cpp generative
programming techniques can deliver efficient automatically
generated codes for such routines.
The generated kernels are competitive with existing,
hand-tuned library kernels with a very low programming effort
compared to writing assembly code.
\\

The results of this chapter have been published in
Jules P\'enuchot, Joel Falcou, Amal Khabou,
"\textit{Modern Generative Programming for Optimizing Small Matrix-Vector
Multiplication}",
2018 International Conference on High Performance Computing Simulation (HPCS)
\cite{hpcs2018-matvec}.

\section{
  Context
}

The efforts of optimizing the performance of \gls{blas} routines
fall into two main directions. The first direction is about
writing very specific assembly code. This is the case for
almost all the vendor libraries including Intel MKL\cite{hpcs1},
AMD ACML\cite{hpcs2} etc. To provide the users with efficient \gls{blas}
routines, the vendors usually implement their own routines
for their own hardware using assembly code with specific
optimizations which is a low level solution that gives the
developers full control over both the instruction scheduling
and the register allocation. This makes these routines highly
architecture dependent and needing considerable efforts to
maintain the performance portability on the new architecture
generations. Moreover, the developed source codes are generally
complex. The second direction is based on using modern
generative programming techniques which have the advantage
of being independent from the architecture specifications and
as a consequence easy to maintain since it is the same source
code which is used to automatically generate a specific code
for a specific target architecture. With respect to the second
direction, some solutions have been proposed in recent years.
However, they only solve partially the trade-off between the
abstraction level and the efficiency of the generated codes.
This is for example the case of the approach followed by
the Formal Linear Algebra Methods Environment (FLAME)
with the Libflame library\cite{hpcs3}. Thus, it offers a framework to
develop dense linear solvers using algorithmic skeletons\cite{hpcs4}
and an API which is more user-friendly than LAPACK, giving
satisfactory performance results. A more generic approach is
the one followed in recent years by \cpp libraries built around
expression templates\cite{hpcs5} or other generative programming\cite{hpcs6}
principles. In this section, we will focus on such an approach.
To show the interest of this approach, we consider as
example the matrix-vector multiplication kernel (gemv) which
is crucial for the performance of both linear solvers and eigen
and singular value problems. Achieving performance running
a matrix-vector multiplication kernel on small problems is
challenging as we can see through the current state-of-the-art
implementation results. Moreover, the different CPU architectures
bring further challenges for its design and optimization.

The quality and performance of \gls{blas} like code require
the ability to write tight and highly-optimized code. If the
raw assembly of low-level C has been the language of choice
for decades, our position is that the proper use of the zero abstraction
property of \cpp can lead to the design of a single,
generic yet efficient code base for many \gls{blas} like functions.
To do so, we will rely on two main elements: a set of code generation techniques
stemmed from metaprogramming and a proper \cpp \gls{simd} layer.

\section{
  Code generation via metaprogramming
}

As we saw earlier, metaprogramming is used in \cpp \cite{hpcs9},D \cite{hpcs10},
OCaml\cite{hpcs11} or Haskell\cite{hpcs12}. A subset of basic notions emerges:

\begin{itemize}
\item
Code fragment generation:
Any metaprogrammable
language has a way to build an object that represents a
piece of code. The granularity of this fragment of code
may vary –ranging from statement to a complete class
definition–but the end results is the same: to provide
an entry level entity to the metaprogramming system.
In some languages, such as MetaOCaml for example,
a special syntax can be provided to construct such
fragment. In some others, code fragment are represented
as a string containing the code itself.

\item
Code processing:
Code fragments are meant to
be combined, introspected or replicated in order to
let the developer rearrange these fragments and as a
consequence to provide a given service. Those processing
steps can be done either via a special syntax construct,
like the MetaOCaml code aggregation operator, or can
use a similar syntax than a regular code.

\end{itemize}

Now let's focus on language-based metaprogramming
techniques so that the proposed method can
be used in various compilers and OS settings as long as the
compiler follows a given standard.
With the standard \cpp revision in 2014 and 2017, this
strategy was renewed with three new \cpp features:

\begin{itemize}
\item
Polymorphic, variadic anonymous functions: \cpp11
introduced the notion of local, anonymous functions
(also known as lambda functions) in the language.
Their primary goal was to simplify the use of standard
algorithms by providing a compact syntax to define a
function in a local scope, hence raising code locality.
\cpp14 added the support for polymorphic lambdas, \ie
anonymous functions behaving like function templates
by accepting arguments of arbitrary types, and variadic
lambdas, \ie anonymous functions accepting a list of
arguments of arbitrary size. Listing \ref{lst:polymorphic-lambda}
demonstrates this particular feature.

\begin{lstlisting}[
  language=c++,
  caption=Sample polymorphic lambda definition,
  label=lst:polymorphic-lambda
]{}
// Variadic function object building array
auto array_from =
    [](auto... values) {
      // sizeof... retrieves the
      // number of arguments
      return std::array<double,
                        sizeof...(values)>{
          values...};
    }
// Build an array of 4 double
auto data = array_from(1, 2, 3., 4.5f);
\end{lstlisting}

\item
Fold expressions: \cpp11 introduced the \lstinline{...} operator
which was able to enumerate a variadic list of functions
or template arguments in a comma-separated code
fragment. Its main use was to provide the syntactic
support required to write a code with variadic template
arguments. However, Niebler and Parent showed that
this can be used to generate far more complex code
when paired with other language constructs. Both
code replication and a crude form of code unrolling
were possible. However, it required the use of some
counter-intuitive structure. \cpp17 extends this notation
to work with an arbitrary binary operator.
Listing \ref{lst:cpp17-fold-exp} illustrates
an example for this feature.

\begin{lstlisting}[
  language=c++,
  caption=\cpp17 fold expressions,
  label=lst:cpp17-fold-exp
]{}
template<typename... Args>
auto reduce(Args&&... args) {
  // Automatically unroll the args into a sum
  return (args + ...);
}
\end{lstlisting}

\paragraph{Tuples} Introduced by \cpp11, tuple usage in \cpp was
simplified by providing new tuple related functions in
\cpp17 that make tuple a fully programmable struct-like
entity. The transition between tuple and structure is then
handled via the new structured binding syntax that allow
the compile-time deconstruction of structures and tuples
in a set of disjoint variables, thus making interface
dealing with tuples easier to use.
Listing \ref{lst:tuple-struct-bind} gives an
example about tuples.

\begin{lstlisting}[
  language=c++,
  caption=Tuple and structured bindings,
  label=lst:tuple-struct-bind
]{}
// Build a tuple from values
auto data = std::make_tuple(3.f, 5, "test");

// Direct access to tuple data
std::get<0>(data) = 6.5f;

// Structured binding access
auto&[a,b,c] = data;

// Add 3 to the second tuple's element
b += 3;
\end{lstlisting}

\end{itemize}

\section{
  C++ performance layer
}

The main strategies to get efficient small-scale \gls{blas}
functions are on one hand the usage of the specific
instructions set (mainly \gls{simd} instructions set) of the target
architecture that is vectorization and on the other hand the
controlled unrolling of the inner loop to ensure proper register
and pipeline usage.

\paragraph{
  Vectorization
}

Vectorization can be achieved either
using the specific instructions set of each vendor or
by relying on auto-vectorization. In our case, to ensure
homogeneous performances across the different target
architectures, we relied on the Boost.SIMD\cite{hpcs17} package
to generates \gls{simd} code for all our architectures.
Boost.SIMD relies on \cpp metaprogramming to act as
a zero-cost abstraction over \gls{simd} operations in a large
number of contexts. The \gls{simd} code is then as easily
written as a scalar version of the code and deliver 95\%
to 99\% of the peak performances for the L1 cache hot
data. The main advantage of the Boost.SIMD package
lies in the fact that both scalar and \gls{simd} code can
be expressed with the same subset of functions. The
vector nature of the operations will be triggered by
the use of a dedicated type – pack – representing the
best hardware register type for a given type on a given
architecture that leads to optimized code generation.

Listing \ref{lst:boostsimd-sample} demonstrates how a naive
implementation of a vectorized dot product can simply be
derived from using Boost.SIMD types and range adapters,
polymorphic lambdas and standard algorithm.

\clearpage%

Note how the Boost.SIMD abstraction shields the end
user to have to handle any architecture specific idioms
and how it integrates with standard algorithms, hence
simplifying the design of more complex algorithms.
Another point is that, by relying on higher-level library
instead of \gls{simd} pragma, Boost.SIMD guarantees
the quality of the vectorization across compilers and
compiler versions. It also leads to a cleaner and easier to
maintain codes, relying only on standard \cpp constructs.

\begin{lstlisting}[
  language=c++,
  caption=Sample Boost.SIMD code,
  label=lst:boostsimd-sample
]{}

template <typename T>
auto simd_dot(T *in1, T *in2, std::size_t count) {
  // Adapt [in,in+count[ as a vectorizable range
  auto r1 = simd::segmented_range(in1, in1 + count);
  auto r2 = simd::segmented_range(in2, in2 + count);

  // Extract sub-ranges
  auto h1 = r1.head, h2 = r2.head;
  auto t1 = r1.tail, t2 = r2.tail;

  // sum and product polymorphic functions
  auto sum = [](auto a, auto b) { return a + b; };
  auto prod = [](auto r, auto v) { return r * v; };

  // Process vectorizable & scalar sub-ranges
  auto vr = std::transform_reduce(
      h1.begin(), h1.end(), h2.begin(), prod, sum,
      simd::pack<T>{});

  auto sr = std::transform_reduce(
      t1.begin(), t1.end(), t2.begin(), prod, sum,
      T{});

  // Compute final dot product
  return sr + simd::sum(vr);
}
\end{lstlisting}

\paragraph{Loop unrolling} The notion of unrolling requires
a proper abstraction. Loop unrolling requires three
elements: the code fragment to repeat, the code
replication process and the iteration space declaration.
Their mapping into \cpp code is as follows:

\begin{itemize}
\item
The code fragment in itself, which represents
the original loop body, is stored inside a
polymorphic lambda function. This lambda
function will takes a polymorphic argument which
will represent the current value of the iteration
variable. This value is passed as an instance of
\lstinline{std::integral_constant} which allows to
turn an arbitrary compile-time constant integer into
a type. By doing so, we are able to propagate the
constness of the iteration variable as far as possible
inside the code fragment of the loop body.

\item
The unrolling process itself relies on the fold
expression mechanism. By using the sequencing
operator, also known as operator comma, the
compiler can unroll arbitrary expressions separated
by the comma operator. The comma operator will
take care of the order of evaluation and behave as
an unrollable statement.

\item
The iteration space need to be specified
as an entity supporting expansion via \lstinline{...}
and containing the actual value of the
iteration space. Standard \cpp provides the
\lstinline{std::integral_sequence<N...>} class that
acts as a variadic container of integral constant. It
can be generated via one helper meta-function such
as \lstinline{std::make_integral_sequence<T,N>}
and passed directly to a variadic function template.
All these elements can then be combined into a
\lstinline{for_constexpr} function detailed in
listing \ref{lst:ct-unroller}.
\end{itemize}

\begin{lstlisting}[
  language=c++,
  caption=Compile-time unroller,
  label=lst:ct-unroller
]{}
template <int Start, int D, typename Body,
          int... Step>
void for_constexpr(
    Body body, std::integer_sequence<int, Step...>,
    std::integral_constant<int, Start>,
    std::integral_constant<int, D>) {
  (body(std::integral_constant<int,
                               Start + D * Step>{}),
   ...);
}

template <int Start, int End, int D = 1,
          typename Body>
void for_constexpr(Body body) {
  constexpr auto size = End - Start;
  for_constexpr(
      std::move(body),
      std::make_integer_sequence<int, size>{},
      std::integral_constant<int, Start>{},
      std::integral_constant<int, D>{});
}
\end{lstlisting}

The function proceed to compute the proper
integral constant sequence from the Start,
End and D compile-time integral constant. As
\lstinline{std::integral_sequence<N...>} enumerates values
from 0 to N , we need to pass the start index and iteration
1 pragma are compiler-dependent and can be ignored
increment as separate constants. The actual index is then
computed at the unrolling site. To prevent unwanted copies
and ensure inlining, all elements are passed to the function
as a rvalue-reference or a universal reference.

A sample usage of the \lstinline{for_constexpr} function is given
in listing \ref{lst:tuple-printing} in a function printing
every element from a \lstinline{std::tuple}.

\begin{lstlisting}[
  language=c++,
  caption=Tuple member walkthrough via compile-time unrolling,
  label=lst:tuple-printing
]{}
template<typename Tuple>
void print_tuple(Tuple const& t) {
  constexpr auto size = std::tuple_size<Tuple>::value;
  for_constexpr<0, size>([&](auto i) {
    std::cout << std::get<i>(t) << "\n";
  });
}
\end{lstlisting}

Note that this implementation exposes some interesting
properties:

\begin{itemize}
\item
As \lstinline{for_constexpr} calls are simple function call, they
can be nested in arbitrary manners.

\item
Relying on \lstinline{std::integral_constant} to carry the
iteration index gives access to its automatic conversion
to integer. This means the iteration index can be used in
both compile-time and runtime contexts.

\item
Code generation quality will still be optimized by the
compiler, thus letting all other non-unrolling related optimizations
to be applied.

One can argue about the advantage of such a method
compared to relying on the compiler unrolling algorithm
or using non-standard unrolling pragma. In both cases, our
method ensure that the unrolling is always done at the fullest
extend and does not rely on non-standard extensions.
\end{itemize}

\section{
  Application: the GEMV kernel
}

Level 2 \gls{blas} routines such as \gls{gemv} have a low
computational intensity compared to Level 3 \gls{blas} operations
such as GEMM. For that reason, in many dense linear algebra
algorithms in particular for one sided factorizations such as
Cholesky, LU, and QR decompositions some techniques are
used to accumulate several Level 2 \gls{blas} operations when
possible in order to perform them as one Level 3 \gls{blas}
operation\cite{hpcs18}. However, for the two-sided factorizations,
and despite the use of similar techniques, the fraction of the
Level 2 \gls{blas} floating point operations is still important. For
instance, for both the bidiagonal and tridiagonal reductions,
this fraction is around 50\%\cite{hpcs19}. Thus, having optimized
implementations for these routines on different architectures
remains important to improve the performance of several
algorithms and applications. Moreover, small-scale \gls{blas}
kernels are useful for some batched computations\cite{hpcs20}.

Here, we consider the matrix-vector multiplication routine
for general dense matrices, \gls{gemv}, which performs either
$y := \alpha A x + \beta y$ or $y := \alpha A T x + \beta y$,
where $A$ is an $m \times n$ matrix, $\alpha$ and $\beta$ are scalars,
and $y$ and $x$ are vectors. In this section,
we focus on matrices of small sizes ranging from
4 to 512 as this range of sizes encompasses the size of
most L1 cache memory, thus allowing a maximal throughput
for \gls{simd} computation units. The algorithm we present in
listing \ref{lst:gemv-kernel} is optimized for a column-major matrix.
For space consideration, we will only focus on the core processing of
the computation, \ie the \gls{simd} part, as the computation of
the scalar tail on the last columns and rows can be trivially
inferred from there.

Our optimized code relies on two types representing
statically-sized matrix and vector, namely \lstinline{mat<T,H,W>}
and \lstinline{vec<T,N>}. Those types carry their height and width
as template parameters so that all size related values can be
derived from them. The code shown in listing \ref{lst:gemv-kernel} is made up
of three main steps as detailed in figure \ref{fig:simd-gemv-illustration}:

\begin{figure}[h]
\fontsize{8}{10}\selectfont
\includesvg[width=\textwidth]{images/gemv-fig1}
\caption{
  An example of matrix vector multiplication showing the \gls{simd}/scalar
  computation boundaries. The matrix is $9 \times 9$ of simple precision floats
  so we can put 4 elements per \gls{simd} register.
}
\label{fig:simd-gemv-illustration}
\end{figure}

\begin{enumerate}
\item
The computation of \gls{simd}/scalar boundaries based
on the static size of the matrix and the size of the
current \gls{simd} registers. Those computations are done
in constexpr contexts to ensure their usability in the
upcoming unrolling steps.

\item
A first level of unrolling that takes care of iterating
over all set of columns that are able to fit into \gls{simd}
registers. This unrolling is done so that both the
corresponding columns of the matrix and the elements
of the vector can respectively be loaded and broadcasted
into \gls{simd} registers.

\begin{lstlisting}[
  language=c++,
  caption=Unrolled GEMV kernel,
  label=lst:gemv-kernel
]{}
template <typename T, std::size_t H,
          std::size_t W>
void gemv(mat<T, H, W> &mat, vec<T, W> &vec,
          vec<T, W> &r) {
  using pack_t = bs::pack<T>;
  constexpr auto sz = pack_t::static_size;

  // Separating simd/scalar parts
  constexpr auto c_simd = W - W % sz;
  constexpr auto r_simd = H - H % sz;

  for_constexpr<0, c_simd,sz>( [](auto j) {
    pack_t pvec(&vec[j]);
    pack_t mulp_arr[sz];

    // Broadcasting vectors once and for all
    for_constexpr<0, sz>([&](auto idx) {
      mulp_arr[idx] = simd::broadcast<idx>(pvec);
    });

    // Walk through SIMD rows
    for_constexpr<0, r_simd>([&](auto I) {
      pack_t resp(&res[i + (I * sz)]);

      // Walk through columns
      for_constexpr<0, sz>([&](auto J) {
        pack_t matp(&mat(i + (I * sz), j + J));
        resp += matp * mulp_arr[J];
        simd::store(resp, &r[i + (I * sz)]);
      });
    }

    // Scalar code follows ...
}
\end{lstlisting}

\item
A second level of unrolling that pass through all the
available \gls{simd} registers loadable from a given column.
We rely on an overloaded \lstinline{operator()} on the matrix
to compute the proper position to load from. As usual
with Boost.SIMD, the actual computation is run with
scalar-like syntax using regular operators.
\end{enumerate}

It is important to notice how close the actual unrolled code
is to an equivalent code that would use regular for loops. This
verisimilitude shows that modern metaprogramming matured
enough so that the frontier between regular runtime programming
and compile-time computation and code generation is
very thin. The effort to fix bugs in such code or to upgrade
it to new algorithms is comparable to the effort required
by a regular code. The notion of code fragment detailed
in Section II helps us to encapsulate those complex metaprogramming
cases into an API based on function calls.

\section{
  Performance results of the generated GEMV codes
}

To validate our approach, we consider two main targeted
processor architectures: an x86 Intel processor i5-7200 and an
ARM processor AMD A1100 with Cortex A57. We compare
the performance of the generated \gls{gemv} codes to that of the
\gls{gemv} kernel of the OpenBLAS library based on GotoBLAS2 1.13\cite{hpcs21}.
We use GCC 7.2\cite{hpcs22} with maximal level of optimization.

In the following experiments, we only show results
for simple precision floats with column major data, but we
obtained similar results for the double precision case, as well
as the row major data. All the results displayed below are
obtained using one thread. All those results has been obtained
using Google Benchmark micro-benchmark facilities. Every
experiments have been repeated for a duration of 3s, using
the median time as an approximation of the most frequent
measured time.

\subsection{
  On X86 Intel processor
}

\begin{figure}[h]
\fontsize{8}{10}\selectfont
\includesvg[width=\textwidth]{images/gemv-fig2}
\caption{
  GEMV performance on Intel i5-7200 processor using \gls{simd}
  Extensions set (SSE-4.2)
}
\label{fig:gemv-avx-sse}
\end{figure}

In figure \ref{fig:gemv-avx-sse}, we compare the performance
of our implementation using the \gls{simd} Extensions set SSE 4.2
and a similarly configured OpenBLAS \gls{gemv} kernel. The obtained results show
that the performances of our automatically generated code is
up to 2 times better for matrices of sizes ranging from $4 \times 4$
elements to $16 \times 16$ elements. However, for matrices of size
$32 \times 32$ elements and $64 \times 64$ elements, the OpenBLAS \gls{gemv}
kernel gives a better performances, especially for the $64 \times 64$
case. This is because the OpenBLAS library uses a dedicated
\gls{gemv} kernel with specific optimizations and prefetching
strategies that our generic solution can not emulate. Beyond
this size ($64 \times 64$ elements), the $L1 \rightarrow L2$ cache misses
cause a performance drop for both our generated code and
the OpenBLAS \gls{gemv} kernel. Nevertheless, our generated code
sustains a better throughput for matrices of sizes $128 \times 128$
elements. For matrices of size $256 \times 256$ elements, the register
usage starts to cause spill to the stack, showing that our
solution can not be arbitrarily extended further to larger matrix sizes.

\begin{figure}[h]
\fontsize{8}{10}\selectfont
\includesvg[width=\textwidth]{images/gemv-fig3}
\caption{
  GEMV performance on Intel i5-7200 processor using
  Advanced Vector Extensions (AVX)
}
\label{fig:gemv-avx-bench}
\end{figure}

In figure \ref{fig:gemv-avx-bench}, we compare the performance
of our generated \gls{gemv} code using Advanced Vector Extensions
AVX2 to the performance of a similarly configured OpenBLAS \gls{gemv} kernel.
Again, the performances of our implementation are close
to that of OpenBLAS and are even quite better for matrices of
small sizes ranging from 4 to 16 elements. For example, for a
matrix of size 8 elements,the automatically generated code has
a performance that is 3 times better than the OpenBLAS \gls{gemv}
kernel (15.78 Gflop/s vs 5.06 Gflop/s). Two phenomenons
appear however. The first one is that the increased number
of the AVX registers compared to the SSE ones makes the
effect of register spilling less prevalent. The second one is
that the code generated for the special $64 \times 64$ elements
case\cite{hpcs23} in OpenBLAS has a little advantage compared to
our automatically generated code. Finally, we note the fact
that, for matrices of size above $512 \times 512$ elements, we stop
being competitive due to the large amount of registers our
fully unrolled approach would require.

In both cases, the register pressure is clearly identified as a
limitation. One possible way to fix this issue will be to rely
on partial loop unrolling and using compile-time informations
about architecture to decide the level of unrolling to apply for
a given size on a given architecture.

\subsection{
  On ARM processor
}

The comparison between our automatically generated code
and the ARM OpenBLAS \gls{gemv} kernel is given in figure \ref{fig:gemv-arm-bench}.
Contrary to the x86 Intel processor, we sustain a comparable
yet slightly better throughput than the OpenBLAS \gls{gemv}
kernel. The analysis of the generated assembly code shows
that our method of guiding the compiler and letting it do
fine grained optimizations generates a better code than the
hand-written assembly approach of the OpenBLAS library.

\begin{figure}[h]
\fontsize{8}{10}\selectfont
\includesvg[width=\textwidth]{images/gemv-fig4}
\caption{
  GEMV performance on ARM Cortex A57 processor
}
\label{fig:gemv-arm-bench}
\end{figure}

We exhibit performance drops similar to OpenBLAS due to
$L1 \rightarrow L2$ misses. Register spilling also happens once we reach
$512 \times 512$ elements. The combination of our template based
unrolling and Boost.SIMD shows that it is indeed possible to
generate ARM NEON code from high-level \cpp with zero
abstraction cost.

\section{
  Conclusion
}

This chapter presented the details of generating an optimized
level 2 \gls{blas} routine \gls{gemv}. As a key difference with respect
to highly tuned OpenBLAS routine, our generated code
is designed to give the best performance with a minimum
programming effort for rather small matrices that fit into the
L1 cache. Compared to the best open source \gls{blas} library,
OpenBLAS, the automatically generated \gls{gemv} codes show
competitive speedups for most of the matrix sizes tested in
this section, that is for sizes ranging from 4 to 512. Therefore
through this section and the example of the level 2 \gls{blas}
routine \gls{gemv}, we showed that it is possible to employ modern
generative programming techniques for the implementation
of dense linear algebra routines that are highly optimized for
small matrix sizes.

Now that we know that \gls{tmp} is a viable technique for generating
portable high performance kernels that can compete with hand written assembly,
we will dive deeper into newer \cpp metaprogramming techniques and apply them
for compile-time parsing and code generation.

% One of our next steps is to provide such metaprogrammed
% code generation process to tackle larger matrix sizes by precomputing
% an optimal tiling availability of prefetch and architecture,
% turning large-scale \gls{gemv} into an optimized sequence of
% statically sized small-scale GEMVs. Such a technique is also naturally
% applicable to more complex algorithms, such the matrix-matrix
% multiplication\cite{matrix-matrix} where tiling is paramount,
% or LAPACK-level functions where the compile-time optimization
% may lead to an easier to parallelize solvers or decompositions.

\end{document}
